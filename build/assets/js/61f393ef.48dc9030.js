"use strict";(globalThis.webpackChunkphysical_ai_humanoid_robotics_book=globalThis.webpackChunkphysical_ai_humanoid_robotics_book||[]).push([[824],{2617:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>a,default:()=>m,frontMatter:()=>t,metadata:()=>o,toc:()=>c});const o=JSON.parse('{"id":"module-4-vision-language-action/chapter-4-2-whisper-voice-commands","title":"Whisper Voice Command Processing","description":"Goal","source":"@site/docs/module-4-vision-language-action/chapter-4-2-whisper-voice-commands.md","sourceDirName":"module-4-vision-language-action","slug":"/module-4-vision-language-action/chapter-4-2-whisper-voice-commands","permalink":"/physical-ai-humanoid-robotics-book/docs/module-4-vision-language-action/chapter-4-2-whisper-voice-commands","draft":false,"unlisted":false,"editUrl":"https://github.com/your-org/physical-ai-humanoid-robotics-book/tree/main/docs/module-4-vision-language-action/chapter-4-2-whisper-voice-commands.md","tags":[],"version":"current","frontMatter":{"id":"chapter-4-2-whisper-voice-commands","title":"Whisper Voice Command Processing","sidebar_label":"Whisper Voice Command Processing"},"sidebar":"book","previous":{"title":"Vision-Language-Action Systems","permalink":"/physical-ai-humanoid-robotics-book/docs/module-4-vision-language-action/chapter-4-1-vla-systems"},"next":{"title":"LLM Cognitive Planning","permalink":"/physical-ai-humanoid-robotics-book/docs/module-4-vision-language-action/chapter-4-3-llm-cognitive-planning"}}');var r=i(4848),s=i(8453);const t={id:"chapter-4-2-whisper-voice-commands",title:"Whisper Voice Command Processing",sidebar_label:"Whisper Voice Command Processing"},a="Whisper Voice Command Processing",l={},c=[{value:"Goal",id:"goal",level:2},{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Overview",id:"overview",level:2},{value:"Key Concepts",id:"key-concepts",level:2},{value:"Step-by-Step Breakdown",id:"step-by-step-breakdown",level:2},{value:"Code Examples",id:"code-examples",level:2},{value:"Diagrams",id:"diagrams",level:2},{value:"Case Study",id:"case-study",level:2},{value:"References",id:"references",level:2},{value:"Review Questions",id:"review-questions",level:2},{value:"Practical Exercises",id:"practical-exercises",level:2}];function d(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"whisper-voice-command-processing",children:"Whisper Voice Command Processing"})}),"\n",(0,r.jsx)(n.h2,{id:"goal",children:"Goal"}),"\n",(0,r.jsx)(n.p,{children:"Implement voice command processing for humanoid robots using OpenAI's Whisper model, enabling natural language interaction through speech recognition and command interpretation."}),"\n",(0,r.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Understand the Whisper model architecture and capabilities"}),"\n",(0,r.jsx)(n.li,{children:"Configure Whisper for real-time voice command processing"}),"\n",(0,r.jsx)(n.li,{children:"Integrate Whisper with humanoid robot control systems"}),"\n",(0,r.jsx)(n.li,{children:"Implement voice command interpretation and execution"}),"\n",(0,r.jsx)(n.li,{children:"Optimize Whisper for robot hardware constraints"}),"\n",(0,r.jsx)(n.li,{children:"Evaluate voice recognition performance in robotic applications"}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,r.jsx)(n.p,{children:"Voice command processing is a critical component of human-robot interaction, allowing users to control humanoid robots through natural speech. OpenAI's Whisper model provides state-of-the-art speech recognition capabilities that can be adapted for robotic applications. By integrating Whisper with humanoid robot systems, we can enable more intuitive and accessible robot control, allowing users to issue commands in natural language that the robot can understand and execute."}),"\n",(0,r.jsx)(n.h2,{id:"key-concepts",children:"Key Concepts"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Whisper Model"}),": OpenAI's automatic speech recognition (ASR) model"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Speech Recognition"}),": Converting audio to text"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Voice Command Interpretation"}),": Understanding user intent from recognized text"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Real-time Processing"}),": Handling audio streams in real-time"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Acoustic Environment"}),": Adapting to robot's operating environment"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Command Mapping"}),": Connecting recognized commands to robot actions"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Privacy Considerations"}),": Handling voice data appropriately"]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"step-by-step-breakdown",children:"Step-by-Step Breakdown"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Whisper Model Setup"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Install and configure Whisper model"}),"\n",(0,r.jsx)(n.li,{children:"Select appropriate model size for hardware constraints"}),"\n",(0,r.jsx)(n.li,{children:"Set up audio input processing pipeline"}),"\n",(0,r.jsx)(n.li,{children:"Configure language and accent settings"}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Audio Input Configuration"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Set up microphone array for robot"}),"\n",(0,r.jsx)(n.li,{children:"Configure audio sampling parameters"}),"\n",(0,r.jsx)(n.li,{children:"Implement noise reduction and filtering"}),"\n",(0,r.jsx)(n.li,{children:"Handle audio preprocessing for Whisper"}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Real-time Processing Pipeline"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Implement audio streaming to Whisper"}),"\n",(0,r.jsx)(n.li,{children:"Configure buffer management for real-time processing"}),"\n",(0,r.jsx)(n.li,{children:"Set up transcription and command extraction"}),"\n",(0,r.jsx)(n.li,{children:"Optimize for low-latency response"}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Command Interpretation"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Parse recognized text for robot commands"}),"\n",(0,r.jsx)(n.li,{children:"Implement natural language understanding"}),"\n",(0,r.jsx)(n.li,{children:"Create command-to-action mappings"}),"\n",(0,r.jsx)(n.li,{children:"Handle ambiguous or unclear commands"}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Robot Integration"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Connect voice processing to robot control systems"}),"\n",(0,r.jsx)(n.li,{children:"Implement safety checks for voice commands"}),"\n",(0,r.jsx)(n.li,{children:"Configure feedback and confirmation mechanisms"}),"\n",(0,r.jsx)(n.li,{children:"Handle multi-turn conversations"}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Performance Optimization"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Optimize Whisper for robot hardware"}),"\n",(0,r.jsx)(n.li,{children:"Implement caching and preloading strategies"}),"\n",(0,r.jsx)(n.li,{children:"Configure model quantization for efficiency"}),"\n",(0,r.jsx)(n.li,{children:"Validate recognition accuracy and response times"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"code-examples",children:"Code Examples"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"# Example Whisper voice command processing for humanoid robot\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nfrom sensor_msgs.msg import AudioData\nfrom geometry_msgs.msg import Twist\nfrom rclpy.qos import QoSProfile, HistoryPolicy, ReliabilityPolicy\nimport numpy as np\nimport torch\nimport whisper\nimport speech_recognition as sr\nimport pyaudio\nimport wave\nimport threading\nimport queue\nimport time\nfrom typing import Dict, List, Optional, Tuple\nimport json\nimport re\n\nclass WhisperVoiceCommandNode(Node):\n    def __init__(self):\n        super().__init__('whisper_voice_command')\n\n        # Initialize Whisper model\n        self.whisper_model = None\n        self.load_whisper_model()\n\n        # Audio processing parameters\n        self.audio_params = {\n            'sample_rate': 16000,  # Hz\n            'chunk_size': 1024,    # samples\n            'channels': 1,         # mono\n            'format': pyaudio.paInt16,\n            'buffer_size': 4096    # samples\n        }\n\n        # Voice command parameters\n        self.command_params = {\n            'wake_word': 'robot',\n            'confidence_threshold': 0.7,\n            'command_timeout': 5.0,  # seconds\n            'max_command_length': 100  # characters\n        }\n\n        # Audio buffer and processing\n        self.audio_buffer = queue.Queue()\n        self.recording_active = False\n        self.listening_for_command = False\n        self.last_transcription = \"\"\n\n        # Initialize publishers\n        self.command_pub = self.create_publisher(String, '/voice/command', 10)\n        self.status_pub = self.create_publisher(String, '/voice/status', 10)\n        self.response_pub = self.create_publisher(String, '/voice/response', 10)\n\n        # Initialize subscribers\n        self.audio_sub = self.create_subscription(\n            AudioData, '/audio/data', self.audio_callback, 10\n        )\n\n        # Initialize robot control publisher\n        self.cmd_vel_pub = self.create_publisher(Twist, '/cmd_vel', 10)\n\n        # Voice command mapping\n        self.command_mapping = {\n            'move forward': self.move_forward,\n            'move backward': self.move_backward,\n            'turn left': self.turn_left,\n            'turn right': self.turn_right,\n            'stop': self.stop_robot,\n            'walk': self.start_walking,\n            'halt': self.stop_robot,\n            'dance': self.perform_dance,\n            'hello': self.greet_user,\n            'help': self.provide_help\n        }\n\n        # Start audio processing thread\n        self.audio_thread = threading.Thread(target=self.audio_processing_loop, daemon=True)\n        self.audio_thread.start()\n\n        # Timer for periodic processing\n        self.process_timer = self.create_timer(0.1, self.periodic_processing)\n\n        self.get_logger().info('Whisper Voice Command node initialized')\n\n    def load_whisper_model(self):\n        \"\"\"Load Whisper model for speech recognition\"\"\"\n        try:\n            # Load a smaller model for real-time processing on robot hardware\n            # Options: 'tiny', 'base', 'small', 'medium', 'large'\n            model_size = 'tiny'  # Use tiny for better performance on robot hardware\n            self.whisper_model = whisper.load_model(model_size)\n            self.get_logger().info(f'Whisper model ({model_size}) loaded successfully')\n        except Exception as e:\n            self.get_logger().error(f'Error loading Whisper model: {e}')\n            # Fallback: try loading a CPU-compatible model\n            try:\n                self.whisper_model = whisper.load_model(model_size, device='cpu')\n                self.get_logger().info(f'Whisper model ({model_size}) loaded on CPU')\n            except Exception as e2:\n                self.get_logger().error(f'Error loading Whisper model on CPU: {e2}')\n\n    def audio_callback(self, msg: AudioData):\n        \"\"\"Handle incoming audio data from robot's microphone\"\"\"\n        try:\n            # Convert audio data to numpy array\n            audio_array = np.frombuffer(msg.data, dtype=np.int16)\n\n            # Add to processing buffer\n            self.audio_buffer.put(audio_array)\n\n            # Update status\n            status_msg = String()\n            status_msg.data = f'Audio received: {len(audio_array)} samples'\n            self.status_pub.publish(status_msg)\n\n        except Exception as e:\n            self.get_logger().error(f'Error processing audio callback: {e}')\n\n    def audio_processing_loop(self):\n        \"\"\"Continuous audio processing loop in separate thread\"\"\"\n        audio_frames = []\n        buffer_duration = 2.0  # Process 2 seconds of audio at a time\n\n        while rclpy.ok():\n            try:\n                # Collect audio frames until buffer is full\n                while len(audio_frames) * self.audio_params['chunk_size'] < \\\n                      buffer_duration * self.audio_params['sample_rate']:\n                    try:\n                        # Get audio data from buffer with timeout\n                        audio_chunk = self.audio_buffer.get(timeout=0.1)\n                        audio_frames.append(audio_chunk)\n                    except queue.Empty:\n                        continue\n\n                # Concatenate audio frames\n                if audio_frames:\n                    full_audio = np.concatenate(audio_frames)\n\n                    # Process with Whisper if model is loaded\n                    if self.whisper_model is not None:\n                        try:\n                            # Convert to float32 and normalize\n                            audio_float = full_audio.astype(np.float32) / 32768.0\n\n                            # Transcribe audio using Whisper\n                            result = self.whisper_model.transcribe(\n                                audio_float,\n                                language='en',\n                                task='transcribe'\n                            )\n\n                            transcription = result['text'].strip()\n\n                            if transcription and transcription != self.last_transcription:\n                                self.last_transcription = transcription\n                                self.process_transcription(transcription)\n\n                        except Exception as e:\n                            self.get_logger().error(f'Error in Whisper transcription: {e}')\n\n                    # Clear processed frames\n                    audio_frames = []\n\n            except Exception as e:\n                self.get_logger().error(f'Error in audio processing loop: {e}')\n                time.sleep(0.1)  # Brief pause to prevent busy waiting\n\n    def process_transcription(self, transcription: str):\n        \"\"\"Process the transcribed text for commands\"\"\"\n        self.get_logger().info(f'Transcribed: \"{transcription}\"')\n\n        # Publish raw transcription\n        raw_msg = String()\n        raw_msg.data = transcription\n        self.command_pub.publish(raw_msg)\n\n        # Check for wake word if needed (simplified)\n        if self.command_params['wake_word'] in transcription.lower():\n            # Extract command after wake word\n            command = self.extract_command(transcription)\n        else:\n            command = transcription\n\n        # Process the command\n        if command:\n            self.execute_voice_command(command)\n\n    def extract_command(self, transcription: str) -> str:\n        \"\"\"Extract command from transcription, removing wake word\"\"\"\n        wake_word = self.command_params['wake_word']\n        command = transcription.lower().replace(wake_word, '').strip()\n        return command\n\n    def execute_voice_command(self, command: str):\n        \"\"\"Execute voice command by mapping to robot action\"\"\"\n        try:\n            # Normalize command text\n            normalized_command = self.normalize_command(command)\n\n            # Find matching command in mapping\n            matched_command = self.find_matching_command(normalized_command)\n\n            if matched_command:\n                # Execute the matched command\n                action_func = self.command_mapping[matched_command]\n                success = action_func()\n\n                # Publish response\n                response_msg = String()\n                if success:\n                    response_msg.data = f'Executed: {matched_command}'\n                else:\n                    response_msg.data = f'Failed to execute: {matched_command}'\n                self.response_pub.publish(response_msg)\n\n                self.get_logger().info(f'Executed command: {matched_command}')\n            else:\n                # Command not recognized\n                response_msg = String()\n                response_msg.data = f'Command not recognized: {command}'\n                self.response_pub.publish(response_msg)\n\n                self.get_logger().warn(f'Unrecognized command: {command}')\n\n        except Exception as e:\n            self.get_logger().error(f'Error executing voice command: {e}')\n\n    def normalize_command(self, command: str) -> str:\n        \"\"\"Normalize command text for matching\"\"\"\n        # Remove punctuation and extra whitespace\n        normalized = re.sub(r'[^\\w\\s]', ' ', command.lower())\n        normalized = ' '.join(normalized.split())  # Remove extra spaces\n        return normalized\n\n    def find_matching_command(self, command: str) -> Optional[str]:\n        \"\"\"Find the best matching command from the command mapping\"\"\"\n        # Exact match first\n        if command in self.command_mapping:\n            return command\n\n        # Partial match with fuzzy logic\n        for mapped_command in self.command_mapping.keys():\n            if mapped_command in command or command in mapped_command:\n                return mapped_command\n\n        # Check for similar commands\n        for mapped_command in self.command_mapping.keys():\n            if self.strings_similar(command, mapped_command, threshold=0.6):\n                return mapped_command\n\n        return None\n\n    def strings_similar(self, str1: str, str2: str, threshold: float = 0.6) -> bool:\n        \"\"\"Check if two strings are similar using simple ratio\"\"\"\n        # Simple word overlap approach\n        words1 = set(str1.split())\n        words2 = set(str2.split())\n\n        if not words1 or not words2:\n            return False\n\n        intersection = words1.intersection(words2)\n        union = words1.union(words2)\n\n        similarity = len(intersection) / len(union)\n        return similarity >= threshold\n\n    def periodic_processing(self):\n        \"\"\"Periodic processing for voice command system\"\"\"\n        # This could handle periodic tasks like checking for new commands\n        # or maintaining system status\n        pass\n\n    # Robot action implementations\n    def move_forward(self) -> bool:\n        \"\"\"Move robot forward\"\"\"\n        cmd_vel = Twist()\n        cmd_vel.linear.x = 0.2  # m/s\n        self.cmd_vel_pub.publish(cmd_vel)\n        self.get_logger().info('Moving forward')\n        return True\n\n    def move_backward(self) -> bool:\n        \"\"\"Move robot backward\"\"\"\n        cmd_vel = Twist()\n        cmd_vel.linear.x = -0.2  # m/s\n        self.cmd_vel_pub.publish(cmd_vel)\n        self.get_logger().info('Moving backward')\n        return True\n\n    def turn_left(self) -> bool:\n        \"\"\"Turn robot left\"\"\"\n        cmd_vel = Twist()\n        cmd_vel.angular.z = 0.3  # rad/s\n        self.cmd_vel_pub.publish(cmd_vel)\n        self.get_logger().info('Turning left')\n        return True\n\n    def turn_right(self) -> bool:\n        \"\"\"Turn robot right\"\"\"\n        cmd_vel = Twist()\n        cmd_vel.angular.z = -0.3  # rad/s\n        self.cmd_vel_pub.publish(cmd_vel)\n        self.get_logger().info('Turning right')\n        return True\n\n    def stop_robot(self) -> bool:\n        \"\"\"Stop robot movement\"\"\"\n        cmd_vel = Twist()\n        # Publish zero velocities to stop\n        self.cmd_vel_pub.publish(cmd_vel)\n        self.get_logger().info('Stopping robot')\n        return True\n\n    def start_walking(self) -> bool:\n        \"\"\"Start walking behavior\"\"\"\n        # In a real humanoid robot, this would trigger walking controllers\n        self.get_logger().info('Starting walking behavior')\n        return True\n\n    def perform_dance(self) -> bool:\n        \"\"\"Perform a simple dance routine\"\"\"\n        self.get_logger().info('Performing dance routine')\n        # This would trigger pre-programmed dance movements\n        return True\n\n    def greet_user(self) -> bool:\n        \"\"\"Greet the user\"\"\"\n        self.get_logger().info('Greeting user')\n        # This would trigger audio output or gesture\n        return True\n\n    def provide_help(self) -> bool:\n        \"\"\"Provide help information\"\"\"\n        self.get_logger().info('Providing help information')\n        # This would trigger help response\n        return True\n\nclass WhisperOptimizationNode(Node):\n    \"\"\"Node for optimizing Whisper performance on robot hardware\"\"\"\n\n    def __init__(self):\n        super().__init__('whisper_optimization')\n\n        # Parameters for optimization\n        self.optimization_params = {\n            'model_quantization': True,\n            'batch_size': 1,\n            'use_gpu': False,  # Set based on robot hardware\n            'audio_preprocessing': True,\n            'caching_enabled': True\n        }\n\n        # Initialize optimization components\n        self.initialize_optimization()\n\n        self.get_logger().info('Whisper Optimization node initialized')\n\n    def initialize_optimization(self):\n        \"\"\"Initialize optimization components\"\"\"\n        try:\n            # Initialize audio preprocessing\n            self.setup_audio_preprocessing()\n\n            # Set up model caching if enabled\n            if self.optimization_params['caching_enabled']:\n                self.setup_model_caching()\n\n            # Configure quantization if enabled\n            if self.optimization_params['model_quantization']:\n                self.setup_model_quantization()\n\n        except Exception as e:\n            self.get_logger().error(f'Error initializing optimization: {e}')\n\n    def setup_audio_preprocessing(self):\n        \"\"\"Set up audio preprocessing for better recognition\"\"\"\n        # This would include noise reduction, echo cancellation, etc.\n        self.get_logger().info('Audio preprocessing configured')\n\n    def setup_model_caching(self):\n        \"\"\"Set up model caching for faster inference\"\"\"\n        # Preload frequently used models or model components\n        self.get_logger().info('Model caching configured')\n\n    def setup_model_quantization(self):\n        \"\"\"Set up model quantization for efficiency\"\"\"\n        # This would reduce model size and improve inference speed\n        self.get_logger().info('Model quantization configured')\n\nclass VoiceCommandInterface(Node):\n    \"\"\"Interface for integrating voice commands with other systems\"\"\"\n\n    def __init__(self):\n        super().__init__('voice_command_interface')\n\n        # Publishers for different robot systems\n        self.navigation_pub = self.create_publisher(String, '/navigation/command', 10)\n        self.manipulation_pub = self.create_publisher(String, '/manipulation/command', 10)\n        self.behavior_pub = self.create_publisher(String, '/behavior/command', 10)\n\n        # Subscriber for voice commands\n        self.voice_command_sub = self.create_subscription(\n            String, '/voice/command', self.voice_command_callback, 10\n        )\n\n        self.get_logger().info('Voice Command Interface initialized')\n\n    def voice_command_callback(self, msg: String):\n        \"\"\"Process voice command and route to appropriate system\"\"\"\n        command = msg.data.lower()\n\n        # Route command to appropriate subsystem\n        if any(word in command for word in ['navigate', 'go to', 'move to', 'walk to']):\n            self.navigation_pub.publish(msg)\n        elif any(word in command for word in ['pick', 'grasp', 'take', 'grab', 'lift']):\n            self.manipulation_pub.publish(msg)\n        elif any(word in command for word in ['dance', 'wave', 'greet', 'hello']):\n            self.behavior_pub.publish(msg)\n        else:\n            # General command - might need further processing\n            self.get_logger().info(f'General command: {command}')\n\ndef main(args=None):\n    \"\"\"Main function for Whisper voice command system\"\"\"\n    rclpy.init(args=args)\n\n    # Create nodes\n    voice_node = WhisperVoiceCommandNode()\n    optimization_node = WhisperOptimizationNode()\n    interface_node = VoiceCommandInterface()\n\n    try:\n        # Run the nodes\n        executor = rclpy.executors.MultiThreadedExecutor()\n        executor.add_node(voice_node)\n        executor.add_node(optimization_node)\n        executor.add_node(interface_node)\n\n        executor.spin()\n\n    except KeyboardInterrupt:\n        voice_node.get_logger().info('Shutting down Whisper voice command system')\n    finally:\n        voice_node.destroy_node()\n        optimization_node.destroy_node()\n        interface_node.destroy_node()\n        rclpy.shutdown()\n\n# Alternative implementation using speech_recognition library as fallback\nclass FallbackVoiceRecognitionNode(Node):\n    \"\"\"Fallback voice recognition using speech_recognition library\"\"\"\n\n    def __init__(self):\n        super().__init__('fallback_voice_recognition')\n\n        # Initialize speech recognition\n        self.recognizer = sr.Recognizer()\n        self.microphone = sr.Microphone()\n\n        # Adjust for ambient noise\n        with self.microphone as source:\n            self.recognizer.adjust_for_ambient_noise(source)\n\n        # Publishers\n        self.command_pub = self.create_publisher(String, '/voice/command', 10)\n\n        # Timer for continuous listening\n        self.listen_timer = self.create_timer(1.0, self.continuous_listening)\n\n        self.get_logger().info('Fallback Voice Recognition initialized')\n\n    def continuous_listening(self):\n        \"\"\"Continuously listen for voice commands\"\"\"\n        try:\n            with self.microphone as source:\n                # Listen for audio with timeout\n                audio = self.recognizer.listen(source, timeout=2, phrase_time_limit=5)\n\n            try:\n                # Recognize speech using Google Web Speech API\n                command = self.recognizer.recognize_google(audio)\n                self.get_logger().info(f'Recognized: {command}')\n\n                # Publish command\n                cmd_msg = String()\n                cmd_msg.data = command\n                self.command_pub.publish(cmd_msg)\n\n            except sr.UnknownValueError:\n                self.get_logger().debug('Could not understand audio')\n            except sr.RequestError as e:\n                self.get_logger().error(f'Error with speech recognition service: {e}')\n\n        except sr.WaitTimeoutError:\n            # No speech detected, continue listening\n            pass\n        except Exception as e:\n            self.get_logger().error(f'Error in continuous listening: {e}')\n"})}),"\n",(0,r.jsx)(n.h2,{id:"diagrams",children:"Diagrams"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"Whisper Voice Command Processing Architecture:\n\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Microphone     \u2502    \u2502  Audio          \u2502    \u2502  Whisper        \u2502\n\u2502  Array          \u2502\u2500\u2500\u2500\u25ba\u2502  Preprocessing  \u2502\u2500\u2500\u2500\u25ba\u2502  Model          \u2502\n\u2502  (Robot)        \u2502    \u2502  (Noise, etc.)  \u2502    \u2502  (Transcribe)   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n         \u2502                       \u2502                       \u2502\n         \u25bc                       \u25bc                       \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Raw Audio      \u2502    \u2502  Processed      \u2502    \u2502  Transcribed    \u2502\n\u2502  Data           \u2502    \u2502  Audio          \u2502    \u2502  Text          \u2502\n\u2502  (PCM, etc.)    \u2502    \u2502  (Filtered)     \u2502    \u2502  (Commands)     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                                        \u2502\n                                        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                                        \u2502   Command Interpretation      \u2502\n                                        \u2502   (Natural Language Understanding) \u2502\n                                        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                                        \u2502\n                                        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                                        \u2502   Robot Action Execution      \u2502\n                                        \u2502   (Movement, Manipulation)    \u2502\n                                        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\nVoice Command Processing Pipeline:\n\nAudio Input \u2192 Preprocessing \u2192 Whisper ASR \u2192 NLU \u2192 Command Mapping \u2192 Robot Action\n(Microphone)  (Filter, VAD)   (Speech-to-Text)  (Intent)  (Action)    (Execution)\n\nReal-time Processing Loop:\n\nContinuous Audio \u2500\u2500\u25ba Buffer Management \u2500\u2500\u25ba Batch Processing \u2500\u2500\u25ba Command Execution\n  Capture            (Chunks, Overlap)     (Whisper Inference)   (Robot Control)\n"})}),"\n",(0,r.jsx)(n.h2,{id:"case-study",children:"Case Study"}),"\n",(0,r.jsx)(n.p,{children:'OpenAI\'s Whisper model has been successfully integrated into various robotic systems to enable voice command processing. In one implementation, researchers integrated Whisper with a mobile robot to allow users to control the robot using natural language commands. The system was able to recognize commands like "go to the kitchen" or "pick up the red cup" with high accuracy, even in noisy environments. For humanoid robots, this capability is especially valuable as it enables more natural human-robot interaction, allowing users to communicate with robots as they would with other humans.'}),"\n",(0,r.jsx)(n.h2,{id:"references",children:"References"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://github.com/openai/whisper",children:"OpenAI Whisper Documentation"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://speech-recognition.readthedocs.io/",children:"Speech Recognition in Robotics"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://pyaudio.readthedocs.io/",children:"Real-time Speech Processing"})}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"review-questions",children:"Review Questions"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsx)(n.li,{children:"What are the advantages of using Whisper over traditional speech recognition systems?"}),"\n",(0,r.jsx)(n.li,{children:"How can Whisper be optimized for real-time robotic applications?"}),"\n",(0,r.jsx)(n.li,{children:"What are the challenges in voice command processing for robots?"}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"practical-exercises",children:"Practical Exercises"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsx)(n.li,{children:"Install and configure Whisper on a robot platform"}),"\n",(0,r.jsx)(n.li,{children:"Test voice recognition performance in different acoustic environments"}),"\n",(0,r.jsx)(n.li,{children:"Implement command interpretation for specific robot tasks"}),"\n",(0,r.jsx)(n.li,{children:"Evaluate the system's accuracy and response time"}),"\n"]})]})}function m(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(d,{...e})}):d(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>t,x:()=>a});var o=i(6540);const r={},s=o.createContext(r);function t(e){const n=o.useContext(s);return o.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:t(e.components),o.createElement(s.Provider,{value:n},e.children)}}}]);