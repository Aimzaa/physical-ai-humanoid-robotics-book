"use strict";(globalThis.webpackChunkphysical_ai_humanoid_robotics_book=globalThis.webpackChunkphysical_ai_humanoid_robotics_book||[]).push([[965],{8453:(n,e,a)=>{a.d(e,{R:()=>s,x:()=>r});var i=a(6540);const t={},o=i.createContext(t);function s(n){const e=i.useContext(o);return i.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function r(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(t):n.components||t:s(n.components),i.createElement(o.Provider,{value:e},n.children)}},8605:(n,e,a)=>{a.r(e),a.d(e,{assets:()=>l,contentTitle:()=>r,default:()=>d,frontMatter:()=>s,metadata:()=>i,toc:()=>c});const i=JSON.parse('{"id":"module-3-ai-robot-brain/chapter-3-4-perception-navigation","title":"Perception and Navigation Systems","description":"Goal","source":"@site/docs/module-3-ai-robot-brain/chapter-3-4-perception-navigation.md","sourceDirName":"module-3-ai-robot-brain","slug":"/module-3-ai-robot-brain/chapter-3-4-perception-navigation","permalink":"/physical-ai-humanoid-robotics-book/docs/module-3-ai-robot-brain/chapter-3-4-perception-navigation","draft":false,"unlisted":false,"editUrl":"https://github.com/your-org/physical-ai-humanoid-robotics-book/tree/main/docs/module-3-ai-robot-brain/chapter-3-4-perception-navigation.md","tags":[],"version":"current","frontMatter":{"id":"chapter-3-4-perception-navigation","title":"Perception and Navigation Systems","sidebar_label":"Perception and Navigation Systems"},"sidebar":"book","previous":{"title":"Visual SLAM Implementation","permalink":"/physical-ai-humanoid-robotics-book/docs/module-3-ai-robot-brain/chapter-3-3-vslam-implementation"},"next":{"title":"Synthetic Data Generation","permalink":"/physical-ai-humanoid-robotics-book/docs/module-3-ai-robot-brain/chapter-3-5-synthetic-data-generation"}}');var t=a(4848),o=a(8453);const s={id:"chapter-3-4-perception-navigation",title:"Perception and Navigation Systems",sidebar_label:"Perception and Navigation Systems"},r="Perception and Navigation Systems",l={},c=[{value:"Goal",id:"goal",level:2},{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Overview",id:"overview",level:2},{value:"Key Concepts",id:"key-concepts",level:2},{value:"Step-by-Step Breakdown",id:"step-by-step-breakdown",level:2},{value:"Code Examples",id:"code-examples",level:2},{value:"Diagrams",id:"diagrams",level:2},{value:"Case Study",id:"case-study",level:2},{value:"References",id:"references",level:2},{value:"Review Questions",id:"review-questions",level:2},{value:"Practical Exercises",id:"practical-exercises",level:2}];function p(n){const e={a:"a",code:"code",h1:"h1",h2:"h2",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,o.R)(),...n.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(e.header,{children:(0,t.jsx)(e.h1,{id:"perception-and-navigation-systems",children:"Perception and Navigation Systems"})}),"\n",(0,t.jsx)(e.h2,{id:"goal",children:"Goal"}),"\n",(0,t.jsx)(e.p,{children:"Implement comprehensive perception and navigation systems for humanoid robots using NVIDIA Isaac tools, enabling autonomous movement and environmental understanding."}),"\n",(0,t.jsx)(e.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsx)(e.li,{children:"Understand the integration between perception and navigation in robotics"}),"\n",(0,t.jsx)(e.li,{children:"Configure Isaac's perception packages for humanoid robot navigation"}),"\n",(0,t.jsx)(e.li,{children:"Implement obstacle detection and avoidance algorithms"}),"\n",(0,t.jsx)(e.li,{children:"Create navigation pipelines using Isaac ROS packages"}),"\n",(0,t.jsx)(e.li,{children:"Integrate perception data with path planning and execution"}),"\n",(0,t.jsx)(e.li,{children:"Validate navigation performance in simulation and real environments"}),"\n"]}),"\n",(0,t.jsx)(e.h2,{id:"overview",children:"Overview"}),"\n",(0,t.jsx)(e.p,{children:"Perception and navigation are fundamental capabilities for autonomous humanoid robots. The perception system processes sensor data to understand the environment, while the navigation system uses this information to plan and execute safe paths to goals. NVIDIA Isaac provides GPU-accelerated perception packages and navigation tools specifically designed for complex robotic systems like humanoid robots. This integration enables humanoid robots to operate autonomously in dynamic environments with real-time processing capabilities."}),"\n",(0,t.jsx)(e.h2,{id:"key-concepts",children:"Key Concepts"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Perception Pipeline"}),": Processing sensor data to extract meaningful information"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Obstacle Detection"}),": Identifying and classifying environmental obstacles"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Path Planning"}),": Computing optimal routes to navigation goals"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Local Navigation"}),": Real-time obstacle avoidance and path following"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Global Navigation"}),": Long-term path planning with map awareness"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Sensor Fusion"}),": Combining data from multiple sensors"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Costmaps"}),": Representing environment traversability"]}),"\n"]}),"\n",(0,t.jsx)(e.h2,{id:"step-by-step-breakdown",children:"Step-by-Step Breakdown"}),"\n",(0,t.jsxs)(e.ol,{children:["\n",(0,t.jsxs)(e.li,{children:["\n",(0,t.jsx)(e.p,{children:(0,t.jsx)(e.strong,{children:"Perception System Setup"})}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsx)(e.li,{children:"Configure Isaac ROS perception packages"}),"\n",(0,t.jsx)(e.li,{children:"Set up sensor processing pipelines"}),"\n",(0,t.jsx)(e.li,{children:"Implement object detection and classification"}),"\n",(0,t.jsx)(e.li,{children:"Validate perception accuracy and performance"}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(e.li,{children:["\n",(0,t.jsx)(e.p,{children:(0,t.jsx)(e.strong,{children:"Environment Mapping"})}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsx)(e.li,{children:"Create occupancy grids from sensor data"}),"\n",(0,t.jsx)(e.li,{children:"Implement SLAM for unknown environments"}),"\n",(0,t.jsx)(e.li,{children:"Configure map management and updates"}),"\n",(0,t.jsx)(e.li,{children:"Handle dynamic obstacles in maps"}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(e.li,{children:["\n",(0,t.jsx)(e.p,{children:(0,t.jsx)(e.strong,{children:"Path Planning Algorithms"})}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsx)(e.li,{children:"Implement global planners (A*, Dijkstra, etc.)"}),"\n",(0,t.jsx)(e.li,{children:"Configure local planners for obstacle avoidance"}),"\n",(0,t.jsx)(e.li,{children:"Set up trajectory optimization"}),"\n",(0,t.jsx)(e.li,{children:"Validate path feasibility for humanoid kinematics"}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(e.li,{children:["\n",(0,t.jsx)(e.p,{children:(0,t.jsx)(e.strong,{children:"Navigation Execution"})}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsx)(e.li,{children:"Implement path following controllers"}),"\n",(0,t.jsx)(e.li,{children:"Configure velocity and acceleration limits"}),"\n",(0,t.jsx)(e.li,{children:"Handle navigation recovery behaviors"}),"\n",(0,t.jsx)(e.li,{children:"Integrate with humanoid robot control systems"}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(e.li,{children:["\n",(0,t.jsx)(e.p,{children:(0,t.jsx)(e.strong,{children:"Sensor Fusion Integration"})}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsx)(e.li,{children:"Combine data from cameras, LiDAR, IMU"}),"\n",(0,t.jsx)(e.li,{children:"Implement sensor calibration procedures"}),"\n",(0,t.jsx)(e.li,{children:"Configure fusion algorithms for robustness"}),"\n",(0,t.jsx)(e.li,{children:"Handle sensor failures and fallbacks"}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(e.li,{children:["\n",(0,t.jsx)(e.p,{children:(0,t.jsx)(e.strong,{children:"Performance Optimization"})}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsx)(e.li,{children:"Optimize for real-time processing"}),"\n",(0,t.jsx)(e.li,{children:"Configure computational resource allocation"}),"\n",(0,t.jsx)(e.li,{children:"Implement multi-threading for efficiency"}),"\n",(0,t.jsx)(e.li,{children:"Validate navigation performance metrics"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(e.h2,{id:"code-examples",children:"Code Examples"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:"# Example Isaac ROS perception and navigation system\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import LaserScan, Image, PointCloud2, Imu\nfrom geometry_msgs.msg import Twist, PoseStamped, Point\nfrom nav_msgs.msg import Odometry, OccupancyGrid, Path\nfrom visualization_msgs.msg import Marker, MarkerArray\nfrom std_msgs.msg import Float32\nfrom tf2_ros import TransformException\nfrom tf2_ros.buffer import Buffer\nfrom tf2_ros.transform_listener import TransformListener\nimport tf2_geometry_msgs\nimport numpy as np\nimport cv2\nfrom cv_bridge import CvBridge\nimport math\nfrom typing import List, Tuple, Optional\n\nclass IsaacPerceptionNavigation(Node):\n    def __init__(self):\n        super().__init__('isaac_perception_navigation')\n\n        # Initialize CV bridge\n        self.cv_bridge = CvBridge()\n\n        # TF buffer and listener for coordinate transforms\n        self.tf_buffer = Buffer()\n        self.tf_listener = TransformListener(self.tf_buffer, self)\n\n        # Robot state\n        self.robot_pose = None\n        self.robot_twist = None\n        self.map_resolution = 0.05  # meters per cell\n        self.map_width = 100  # cells\n        self.map_height = 100  # cells\n        self.map_origin_x = 0.0\n        self.map_origin_y = 0.0\n\n        # Navigation parameters\n        self.nav_params = {\n            'max_linear_vel': 0.5,  # m/s\n            'max_angular_vel': 0.5,  # rad/s\n            'min_linear_vel': 0.1,\n            'min_angular_vel': 0.1,\n            'goal_tolerance': 0.2,  # meters\n            'obstacle_threshold': 0.5,  # meters\n            'inflation_radius': 0.3,  # meters\n            'path_resolution': 0.1,  # meters\n        }\n\n        # Initialize perception subscribers\n        self.laser_sub = self.create_subscription(\n            LaserScan, '/scan', self.laser_callback, 10\n        )\n        self.camera_sub = self.create_subscription(\n            Image, '/camera/rgb/image_raw', self.camera_callback, 10\n        )\n        self.odom_sub = self.create_subscription(\n            Odometry, '/odom', self.odom_callback, 10\n        )\n        self.imu_sub = self.create_subscription(\n            Imu, '/imu/data', self.imu_callback, 10\n        )\n\n        # Initialize navigation publishers\n        self.cmd_vel_pub = self.create_publisher(Twist, '/cmd_vel', 10)\n        self.goal_pub = self.create_publisher(PoseStamped, '/goal', 10)\n        self.path_pub = self.create_publisher(Path, '/path', 10)\n        self.map_pub = self.create_publisher(OccupancyGrid, '/map', 10)\n        self.marker_pub = self.create_publisher(MarkerArray, '/visualization_marker_array', 10)\n\n        # Navigation goal\n        self.navigation_goal = None\n        self.current_path = []\n        self.current_waypoint = 0\n\n        # Perception data\n        self.laser_ranges = []\n        self.obstacles = []\n        self.detection_image = None\n\n        # Navigation state\n        self.is_navigating = False\n        self.navigation_active = False\n\n        # Timer for navigation control\n        self.nav_timer = self.create_timer(0.1, self.navigation_control)\n\n        self.get_logger().info('Isaac Perception Navigation system initialized')\n\n    def laser_callback(self, msg: LaserScan):\n        \"\"\"Process laser scan data for obstacle detection\"\"\"\n        try:\n            # Store laser ranges\n            self.laser_ranges = np.array(msg.ranges)\n\n            # Filter out invalid ranges (inf, nan)\n            valid_ranges = self.laser_ranges[\n                (self.laser_ranges > msg.range_min) &\n                (self.laser_ranges < msg.range_max)\n            ]\n\n            # Detect obstacles in laser data\n            self.obstacles = self.detect_obstacles_from_laser(msg)\n\n            # Update occupancy grid based on laser data\n            self.update_occupancy_grid(msg)\n\n        except Exception as e:\n            self.get_logger().error(f'Error in laser callback: {e}')\n\n    def camera_callback(self, msg: Image):\n        \"\"\"Process camera data for visual perception\"\"\"\n        try:\n            # Convert ROS image to OpenCV\n            cv_image = self.cv_bridge.imgmsg_to_cv2(msg, desired_encoding='bgr8')\n\n            # Store image for visualization\n            self.detection_image = cv_image.copy()\n\n            # Process visual perception (object detection, etc.)\n            visual_detections = self.process_visual_perception(cv_image)\n\n            # Update obstacles based on visual data\n            self.update_obstacles_with_visual_data(visual_detections)\n\n        except Exception as e:\n            self.get_logger().error(f'Error in camera callback: {e}')\n\n    def odom_callback(self, msg: Odometry):\n        \"\"\"Update robot pose from odometry\"\"\"\n        self.robot_pose = msg.pose.pose\n        self.robot_twist = msg.twist.twist\n\n    def imu_callback(self, msg: Imu):\n        \"\"\"Process IMU data for orientation and motion\"\"\"\n        # Use IMU data to improve pose estimation\n        # In a real implementation, this would be fused with other sensors\n        pass\n\n    def detect_obstacles_from_laser(self, scan_msg: LaserScan) -> List[Tuple[float, float]]:\n        \"\"\"Detect obstacles from laser scan data\"\"\"\n        obstacles = []\n\n        # Simple threshold-based obstacle detection\n        for i, range_val in enumerate(scan_msg.ranges):\n            if scan_msg.range_min < range_val < scan_msg.range_max:\n                # Convert polar to Cartesian coordinates\n                angle = scan_msg.angle_min + i * scan_msg.angle_increment\n                x = range_val * math.cos(angle)\n                y = range_val * math.sin(angle)\n\n                # Only consider obstacles within threshold\n                if range_val < self.nav_params['obstacle_threshold']:\n                    obstacles.append((x, y))\n\n        return obstacles\n\n    def process_visual_perception(self, image):\n        \"\"\"Process visual perception using Isaac tools\"\"\"\n        # This would typically use Isaac ROS perception packages\n        # like Isaac ROS DetectNet, Isaac ROS Image Pipeline, etc.\n\n        # For demonstration, we'll implement a simple color-based detection\n        hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n\n        # Detect red objects (potential obstacles)\n        lower_red = np.array([0, 100, 100])\n        upper_red = np.array([10, 255, 255])\n        mask1 = cv2.inRange(hsv, lower_red, upper_red)\n\n        lower_red = np.array([170, 100, 100])\n        upper_red = np.array([180, 255, 255])\n        mask2 = cv2.inRange(hsv, lower_red, upper_red)\n\n        mask = mask1 + mask2\n        contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n\n        detections = []\n        for contour in contours:\n            if cv2.contourArea(contour) > 100:  # Filter small contours\n                x, y, w, h = cv2.boundingRect(contour)\n                center_x = x + w // 2\n                center_y = y + h // 2\n                detections.append({\n                    'bbox': (x, y, w, h),\n                    'center': (center_x, center_y),\n                    'area': cv2.contourArea(contour)\n                })\n\n        return detections\n\n    def update_obstacles_with_visual_data(self, detections):\n        \"\"\"Update obstacle list with visual perception data\"\"\"\n        # In a real implementation, this would fuse visual and laser data\n        # For now, we'll just log the visual detections\n        if detections:\n            self.get_logger().debug(f'Visual detections: {len(detections)} objects detected')\n\n    def update_occupancy_grid(self, scan_msg: LaserScan):\n        \"\"\"Update occupancy grid based on laser scan\"\"\"\n        # Create a simple occupancy grid\n        occupancy_grid = OccupancyGrid()\n        occupancy_grid.header.stamp = self.get_clock().now().to_msg()\n        occupancy_grid.header.frame_id = 'map'\n\n        occupancy_grid.info.resolution = self.map_resolution\n        occupancy_grid.info.width = self.map_width\n        occupancy_grid.info.height = self.map_height\n        occupancy_grid.info.origin.position.x = self.map_origin_x\n        occupancy_grid.info.origin.position.y = self.map_origin_y\n\n        # Initialize with unknown (-1)\n        occupancy_grid.data = [-1] * (self.map_width * self.map_height)\n\n        # Update based on laser scan\n        for i, range_val in enumerate(scan_msg.ranges):\n            if scan_msg.range_min < range_val < scan_msg.range_max:\n                angle = scan_msg.angle_min + i * scan_msg.angle_increment\n                x = range_val * math.cos(angle)\n                y = range_val * math.sin(angle)\n\n                # Convert to grid coordinates\n                grid_x = int((x - self.map_origin_x) / self.map_resolution)\n                grid_y = int((y - self.map_origin_y) / self.map_resolution)\n\n                if 0 <= grid_x < self.map_width and 0 <= grid_y < self.map_height:\n                    # Mark as occupied if obstacle detected\n                    if range_val < self.nav_params['obstacle_threshold']:\n                        idx = grid_y * self.map_width + grid_x\n                        occupancy_grid.data[idx] = 100  # occupied\n                    else:\n                        idx = grid_y * self.map_width + grid_x\n                        if occupancy_grid.data[idx] == -1:  # if unknown, mark as free\n                            occupancy_grid.data[idx] = 0  # free\n\n        # Publish the occupancy grid\n        self.map_pub.publish(occupancy_grid)\n\n    def set_navigation_goal(self, x: float, y: float, theta: float = 0.0):\n        \"\"\"Set navigation goal for the robot\"\"\"\n        goal_msg = PoseStamped()\n        goal_msg.header.stamp = self.get_clock().now().to_msg()\n        goal_msg.header.frame_id = 'map'\n        goal_msg.pose.position.x = float(x)\n        goal_msg.pose.position.y = float(y)\n        goal_msg.pose.position.z = 0.0\n\n        # Convert angle to quaternion\n        from tf_transformations import quaternion_from_euler\n        quat = quaternion_from_euler(0, 0, theta)\n        goal_msg.pose.orientation.x = quat[0]\n        goal_msg.pose.orientation.y = quat[1]\n        goal_msg.pose.orientation.z = quat[2]\n        goal_msg.pose.orientation.w = quat[3]\n\n        self.navigation_goal = goal_msg\n        self.is_navigating = True\n        self.navigation_active = True\n\n        self.get_logger().info(f'Set navigation goal to: ({x}, {y})')\n\n    def plan_path(self) -> List[Tuple[float, float]]:\n        \"\"\"Plan path to navigation goal using simple algorithm\"\"\"\n        if not self.robot_pose or not self.navigation_goal:\n            return []\n\n        # Simple path planning - in real implementation this would use A*, Dijkstra, etc.\n        start = (self.robot_pose.position.x, self.robot_pose.position.y)\n        goal = (self.navigation_goal.pose.position.x, self.navigation_goal.pose.position.y)\n\n        # For now, create a straight line path\n        path = [start]\n\n        # Calculate intermediate points\n        steps = int(math.sqrt((goal[0] - start[0])**2 + (goal[1] - start[1])**2) / self.nav_params['path_resolution'])\n        for i in range(1, steps + 1):\n            t = i / steps\n            x = start[0] + t * (goal[0] - start[0])\n            y = start[1] + t * (goal[1] - start[1])\n            path.append((x, y))\n\n        path.append(goal)\n        return path\n\n    def navigation_control(self):\n        \"\"\"Main navigation control loop\"\"\"\n        if not self.navigation_active or not self.robot_pose or not self.navigation_goal:\n            if self.navigation_active:\n                # Stop robot if navigation is active but no goal\n                self.stop_robot()\n            return\n\n        # Plan path if needed\n        if not self.current_path or self.current_waypoint >= len(self.current_path):\n            self.current_path = self.plan_path()\n            self.current_waypoint = 0\n\n            if self.current_path:\n                # Publish path for visualization\n                path_msg = Path()\n                path_msg.header.stamp = self.get_clock().now().to_msg()\n                path_msg.header.frame_id = 'map'\n\n                for point in self.current_path:\n                    pose = PoseStamped()\n                    pose.header.stamp = path_msg.header.stamp\n                    pose.header.frame_id = path_msg.header.frame_id\n                    pose.pose.position.x = point[0]\n                    pose.pose.position.y = point[1]\n                    pose.pose.position.z = 0.0\n                    path_msg.poses.append(pose)\n\n                self.path_pub.publish(path_msg)\n\n        # Execute navigation\n        if self.current_path and self.current_waypoint < len(self.current_path):\n            goal_reached = self.navigate_to_waypoint()\n\n            if goal_reached:\n                self.current_waypoint += 1\n                if self.current_waypoint >= len(self.current_path):\n                    self.get_logger().info('Navigation goal reached!')\n                    self.navigation_active = False\n                    self.stop_robot()\n\n    def navigate_to_waypoint(self) -> bool:\n        \"\"\"Navigate to current waypoint\"\"\"\n        if self.current_waypoint >= len(self.current_path):\n            return True\n\n        target = self.current_path[self.current_waypoint]\n\n        # Calculate distance to target\n        dx = target[0] - self.robot_pose.position.x\n        dy = target[1] - self.robot_pose.position.y\n        distance = math.sqrt(dx**2 + dy**2)\n\n        # Check if reached current waypoint\n        if distance < self.nav_params['goal_tolerance']:\n            return True\n\n        # Calculate desired heading\n        desired_theta = math.atan2(dy, dx)\n\n        # Get current orientation\n        from tf_transformations import euler_from_quaternion\n        current_orientation = [\n            self.robot_pose.orientation.x,\n            self.robot_pose.orientation.y,\n            self.robot_pose.orientation.z,\n            self.robot_pose.orientation.w\n        ]\n        current_euler = euler_from_quaternion(current_orientation)\n        current_theta = current_euler[2]\n\n        # Calculate heading error\n        heading_error = desired_theta - current_theta\n        # Normalize angle to [-pi, pi]\n        while heading_error > math.pi:\n            heading_error -= 2 * math.pi\n        while heading_error < -math.pi:\n            heading_error += 2 * math.pi\n\n        # Create velocity command\n        cmd_vel = Twist()\n\n        # Angular control\n        if abs(heading_error) > 0.1:  # 0.1 rad tolerance\n            cmd_vel.angular.z = max(min(heading_error * 1.0, self.nav_params['max_angular_vel']),\n                                   -self.nav_params['max_angular_vel'])\n        else:\n            # Linear control\n            if distance > self.nav_params['goal_tolerance']:\n                cmd_vel.linear.x = max(min(distance * 0.5, self.nav_params['max_linear_vel']),\n                                      self.nav_params['min_linear_vel'])\n\n        # Check for obstacles\n        if self.detect_obstacles_ahead():\n            # Stop or slow down if obstacles detected\n            cmd_vel.linear.x = 0.0\n            cmd_vel.angular.z = 0.1  # Turn slightly to avoid obstacle\n\n        # Publish velocity command\n        self.cmd_vel_pub.publish(cmd_vel)\n\n        return False\n\n    def detect_obstacles_ahead(self) -> bool:\n        \"\"\"Detect obstacles in front of the robot\"\"\"\n        if not self.laser_ranges:\n            return False\n\n        # Check laser readings in front of the robot (e.g., \xb130 degrees)\n        front_indices = []\n        for i, angle in enumerate(np.arange(\n            self.laser_ranges.shape[0]\n        ) * 0.01 - 1.57):  # Assuming 3.14 FOV, adjust as needed\n            if -0.52 < angle < 0.52:  # \xb130 degrees\n                front_indices.append(i)\n\n        if front_indices:\n            front_ranges = self.laser_ranges[front_indices]\n            min_range = np.min(front_ranges[np.isfinite(front_ranges)])\n            return min_range < self.nav_params['obstacle_threshold']\n\n        return False\n\n    def stop_robot(self):\n        \"\"\"Stop the robot\"\"\"\n        cmd_vel = Twist()\n        cmd_vel.linear.x = 0.0\n        cmd_vel.linear.y = 0.0\n        cmd_vel.linear.z = 0.0\n        cmd_vel.angular.x = 0.0\n        cmd_vel.angular.y = 0.0\n        cmd_vel.angular.z = 0.0\n        self.cmd_vel_pub.publish(cmd_vel)\n\ndef main(args=None):\n    rclpy.init(args=args)\n    perception_nav_node = IsaacPerceptionNavigation()\n\n    try:\n        # Example: Set a navigation goal after startup\n        # This would typically be triggered by a service call or action\n        import time\n        time.sleep(2)  # Wait for connections\n        perception_nav_node.set_navigation_goal(2.0, 2.0, 0.0)  # Example goal\n\n        rclpy.spin(perception_nav_node)\n    except KeyboardInterrupt:\n        perception_nav_node.get_logger().info('Shutting down perception navigation system')\n    finally:\n        perception_nav_node.stop_robot()\n        perception_nav_node.destroy_node()\n        rclpy.shutdown()\n\n# Isaac-specific perception node using Isaac ROS packages\nclass IsaacPerceptionNode(Node):\n    def __init__(self):\n        super().__init__('isaac_perception')\n\n        # This would integrate with Isaac ROS packages like:\n        # - Isaac ROS Stereo DNN for depth estimation\n        # - Isaac ROS DetectNet for object detection\n        # - Isaac ROS Image Pipeline for preprocessing\n        # - Isaac ROS Point Cloud for 3D processing\n\n        self.get_logger().info('Isaac Perception node initialized')\n\n    def setup_isaac_perception_pipeline(self):\n        \"\"\"Setup Isaac-specific perception pipeline\"\"\"\n        # In a real implementation, this would:\n        # 1. Initialize Isaac ROS perception packages\n        # 2. Configure GPU-accelerated processing\n        # 3. Set up sensor fusion\n        # 4. Integrate with navigation stack\n        pass\n"})}),"\n",(0,t.jsx)(e.h2,{id:"diagrams",children:"Diagrams"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{children:"Perception and Navigation System Architecture:\n\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   Sensors       \u2502    \u2502  Perception     \u2502    \u2502  Navigation     \u2502\n\u2502  (Camera,      \u2502\u2500\u2500\u2500\u25ba\u2502  Processing     \u2502\u2500\u2500\u2500\u25ba\u2502  System         \u2502\n\u2502   LiDAR, IMU)   \u2502    \u2502  (GPU)          \u2502    \u2502  (Path Planning \u2502\n\u2502                 \u2502    \u2502                 \u2502    \u2502   & Control)    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n         \u2502                       \u2502                       \u2502\n         \u25bc                       \u25bc                       \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Raw Data       \u2502    \u2502  Processed      \u2502    \u2502  Commands       \u2502\n\u2502  (Images,       \u2502    \u2502  Information    \u2502    \u2502  (Velocities,   \u2502\n\u2502   Scans, etc.)   \u2502    \u2502  (Objects,     \u2502    \u2502   Waypoints)    \u2502\n\u2502                 \u2502    \u2502   Maps, etc.)   \u2502    \u2502                 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\nNavigation Pipeline:\n\nGoal Input \u2192 Global Planner \u2192 Local Planner \u2192 Controller \u2192 Robot\n             (Path Planning)  (Obstacle Avoid)  (Motion)\n\nPerception-Action Loop:\n\nSense Environment \u2192 Process Perception \u2192 Plan Action \u2192 Execute Action \u2192 Sense Again\n(Camera, LiDAR)    (Objects, Maps)     (Path, Goals)  (Motors, etc.)   (Feedback)\n"})}),"\n",(0,t.jsx)(e.h2,{id:"case-study",children:"Case Study"}),"\n",(0,t.jsx)(e.p,{children:"The NVIDIA Isaac team has demonstrated advanced perception and navigation capabilities for humanoid robots using their platform. In one example, a humanoid robot equipped with Isaac's perception stack was able to navigate complex indoor environments, detecting and avoiding both static and dynamic obstacles. The robot used Isaac's GPU-accelerated perception packages to process camera and LiDAR data in real-time, enabling it to build accurate maps of its environment and plan safe paths around obstacles. This system demonstrated the potential for fully autonomous humanoid robots in real-world applications."}),"\n",(0,t.jsx)(e.h2,{id:"references",children:"References"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsx)(e.li,{children:(0,t.jsx)(e.a,{href:"https://github.com/NVIDIA-ISAAC-ROS/isaac_ros_navigation",children:"Isaac ROS Navigation Documentation"})}),"\n",(0,t.jsx)(e.li,{children:(0,t.jsx)(e.a,{href:"https://navigation.ros.org/",children:"ROS 2 Navigation Stack"})}),"\n",(0,t.jsx)(e.li,{children:(0,t.jsx)(e.a,{href:"https://docs.omniverse.nvidia.com/isaacsim/latest/tutorial_ros.html",children:"Isaac Sim Navigation Examples"})}),"\n"]}),"\n",(0,t.jsx)(e.h2,{id:"review-questions",children:"Review Questions"}),"\n",(0,t.jsxs)(e.ol,{children:["\n",(0,t.jsx)(e.li,{children:"How do perception and navigation systems work together in robotics?"}),"\n",(0,t.jsx)(e.li,{children:"What are the key differences between global and local navigation?"}),"\n",(0,t.jsx)(e.li,{children:"How does GPU acceleration benefit perception and navigation systems?"}),"\n"]}),"\n",(0,t.jsx)(e.h2,{id:"practical-exercises",children:"Practical Exercises"}),"\n",(0,t.jsxs)(e.ol,{children:["\n",(0,t.jsx)(e.li,{children:"Implement a simple obstacle detection algorithm"}),"\n",(0,t.jsx)(e.li,{children:"Configure a navigation stack for a humanoid robot model"}),"\n",(0,t.jsx)(e.li,{children:"Test navigation performance with different obstacle configurations"}),"\n",(0,t.jsx)(e.li,{children:"Integrate multiple sensor inputs for improved navigation"}),"\n"]})]})}function d(n={}){const{wrapper:e}={...(0,o.R)(),...n.components};return e?(0,t.jsx)(e,{...n,children:(0,t.jsx)(p,{...n})}):p(n)}}}]);