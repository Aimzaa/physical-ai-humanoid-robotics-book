"use strict";(globalThis.webpackChunkphysical_ai_humanoid_robotics_book=globalThis.webpackChunkphysical_ai_humanoid_robotics_book||[]).push([[802],{1153:(n,e,t)=>{t.r(e),t.d(e,{assets:()=>l,contentTitle:()=>r,default:()=>d,frontMatter:()=>a,metadata:()=>o,toc:()=>c});const o=JSON.parse('{"id":"capstone-project/autonomous-humanoid-implementation","title":"Autonomous Humanoid Implementation","description":"Goal","source":"@site/docs/capstone-project/autonomous-humanoid-implementation.md","sourceDirName":"capstone-project","slug":"/capstone-project/autonomous-humanoid-implementation","permalink":"/physical-ai-humanoid-robotics-book/docs/capstone-project/autonomous-humanoid-implementation","draft":false,"unlisted":false,"editUrl":"https://github.com/your-org/physical-ai-humanoid-robotics-book/tree/main/docs/capstone-project/autonomous-humanoid-implementation.md","tags":[],"version":"current","frontMatter":{"id":"autonomous-humanoid-implementation","title":"Autonomous Humanoid Implementation","sidebar_label":"Autonomous Humanoid Implementation"},"sidebar":"book","previous":{"title":"Voice-to-Action Pipeline Integration","permalink":"/physical-ai-humanoid-robotics-book/docs/module-4-vision-language-action/chapter-4-6-voice-to-action-pipeline"},"next":{"title":"Integration of All Module Concepts","permalink":"/physical-ai-humanoid-robotics-book/docs/capstone-project/integration-concepts"}}');var i=t(4848),s=t(8453);const a={id:"autonomous-humanoid-implementation",title:"Autonomous Humanoid Implementation",sidebar_label:"Autonomous Humanoid Implementation"},r="Autonomous Humanoid Implementation",l={},c=[{value:"Goal",id:"goal",level:2},{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Overview",id:"overview",level:2},{value:"Key Concepts",id:"key-concepts",level:2},{value:"Step-by-Step Breakdown",id:"step-by-step-breakdown",level:2},{value:"Code Examples",id:"code-examples",level:2},{value:"Diagrams",id:"diagrams",level:2},{value:"Case Study",id:"case-study",level:2},{value:"References",id:"references",level:2},{value:"Review Questions",id:"review-questions",level:2},{value:"Practical Exercises",id:"practical-exercises",level:2}];function m(n){const e={a:"a",code:"code",h1:"h1",h2:"h2",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.R)(),...n.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(e.header,{children:(0,i.jsx)(e.h1,{id:"autonomous-humanoid-implementation",children:"Autonomous Humanoid Implementation"})}),"\n",(0,i.jsx)(e.h2,{id:"goal",children:"Goal"}),"\n",(0,i.jsx)(e.p,{children:"Implement a complete autonomous humanoid robot system that integrates all concepts from previous modules, creating a functional voice-controlled robot capable of navigation, manipulation, and interaction."}),"\n",(0,i.jsx)(e.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:"Integrate concepts from all previous modules into a cohesive system"}),"\n",(0,i.jsx)(e.li,{children:"Implement end-to-end autonomous humanoid robot functionality"}),"\n",(0,i.jsx)(e.li,{children:"Configure voice command processing with natural language understanding"}),"\n",(0,i.jsx)(e.li,{children:"Implement navigation and manipulation capabilities"}),"\n",(0,i.jsx)(e.li,{children:"Validate system performance and reliability"}),"\n",(0,i.jsx)(e.li,{children:"Document the complete integrated system architecture"}),"\n"]}),"\n",(0,i.jsx)(e.h2,{id:"overview",children:"Overview"}),"\n",(0,i.jsx)(e.p,{children:"The capstone project brings together all the concepts learned throughout the book to create a complete autonomous humanoid robot system. This integration demonstrates how ROS 2, digital twin simulation, AI-powered perception and planning, and vision-language-action systems work together to create a capable autonomous robot. The system will respond to voice commands, navigate environments, manipulate objects, and interact naturally with humans, showcasing the power of integrated Physical AI systems."}),"\n",(0,i.jsx)(e.h2,{id:"key-concepts",children:"Key Concepts"}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"System Integration"}),": Combining all previous module concepts"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"End-to-End Functionality"}),": Complete autonomous operation"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Real-time Performance"}),": Meeting timing constraints for autonomy"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Robustness"}),": Handling failures and unexpected situations"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"User Experience"}),": Natural and intuitive interaction"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Safety"}),": Ensuring safe robot operation"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Validation"}),": Testing and measuring system performance"]}),"\n"]}),"\n",(0,i.jsx)(e.h2,{id:"step-by-step-breakdown",children:"Step-by-Step Breakdown"}),"\n",(0,i.jsxs)(e.ol,{children:["\n",(0,i.jsxs)(e.li,{children:["\n",(0,i.jsx)(e.p,{children:(0,i.jsx)(e.strong,{children:"System Architecture Design"})}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:"Design complete system architecture integrating all modules"}),"\n",(0,i.jsx)(e.li,{children:"Plan component interfaces and communication"}),"\n",(0,i.jsx)(e.li,{children:"Configure system-level parameters and constraints"}),"\n",(0,i.jsx)(e.li,{children:"Establish safety and performance requirements"}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(e.li,{children:["\n",(0,i.jsx)(e.p,{children:(0,i.jsx)(e.strong,{children:"Core System Integration"})}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:"Integrate ROS 2 communication infrastructure"}),"\n",(0,i.jsx)(e.li,{children:"Connect digital twin simulation with real robot"}),"\n",(0,i.jsx)(e.li,{children:"Implement AI perception and planning systems"}),"\n",(0,i.jsx)(e.li,{children:"Configure voice command processing pipeline"}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(e.li,{children:["\n",(0,i.jsx)(e.p,{children:(0,i.jsx)(e.strong,{children:"Navigation and Mobility"})}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:"Implement autonomous navigation capabilities"}),"\n",(0,i.jsx)(e.li,{children:"Configure humanoid-specific locomotion"}),"\n",(0,i.jsx)(e.li,{children:"Set up obstacle avoidance and path planning"}),"\n",(0,i.jsx)(e.li,{children:"Validate navigation performance"}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(e.li,{children:["\n",(0,i.jsx)(e.p,{children:(0,i.jsx)(e.strong,{children:"Manipulation and Interaction"})}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:"Implement object manipulation capabilities"}),"\n",(0,i.jsx)(e.li,{children:"Configure gripper and arm control"}),"\n",(0,i.jsx)(e.li,{children:"Set up human-robot interaction protocols"}),"\n",(0,i.jsx)(e.li,{children:"Validate manipulation performance"}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(e.li,{children:["\n",(0,i.jsx)(e.p,{children:(0,i.jsx)(e.strong,{children:"Voice and Language Integration"})}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:"Integrate speech recognition and understanding"}),"\n",(0,i.jsx)(e.li,{children:"Connect to cognitive planning systems"}),"\n",(0,i.jsx)(e.li,{children:"Implement natural language interaction"}),"\n",(0,i.jsx)(e.li,{children:"Validate voice command accuracy"}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(e.li,{children:["\n",(0,i.jsx)(e.p,{children:(0,i.jsx)(e.strong,{children:"System Validation and Testing"})}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:"Test complete system functionality"}),"\n",(0,i.jsx)(e.li,{children:"Validate performance metrics"}),"\n",(0,i.jsx)(e.li,{children:"Document system capabilities and limitations"}),"\n",(0,i.jsx)(e.li,{children:"Create comprehensive test procedures"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(e.h2,{id:"code-examples",children:"Code Examples"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:"# Complete autonomous humanoid robot system\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String, Bool\nfrom sensor_msgs.msg import Image, AudioData, JointState, LaserScan\nfrom geometry_msgs.msg import Twist, Pose, Point\nfrom nav_msgs.msg import Odometry\nfrom rclpy.qos import QoSProfile, HistoryPolicy, ReliabilityPolicy\nfrom rclpy.action import ActionServer, ActionClient\nimport numpy as np\nimport torch\nimport whisper\nimport threading\nimport queue\nimport time\nimport json\nfrom typing import Dict, List, Optional, Tuple, Any\nfrom dataclasses import dataclass\nimport openai\nimport cv2\nfrom cv_bridge import CvBridge\n\n@dataclass\nclass HumanoidState:\n    \"\"\"Represents the complete state of the humanoid robot\"\"\"\n    pose: Pose\n    joint_states: List[float]\n    battery_level: float\n    system_status: str\n    current_task: str\n    last_command: str\n    safety_status: str\n\nclass AutonomousHumanoidNode(Node):\n    \"\"\"Main node for autonomous humanoid robot system\"\"\"\n\n    def __init__(self):\n        super().__init__('autonomous_humanoid')\n\n        # Initialize CV bridge\n        self.cv_bridge = CvBridge()\n\n        # Initialize robot state\n        self.robot_state = HumanoidState(\n            pose=Pose(),\n            joint_states=[0.0] * 20,  # Example: 20 joints\n            battery_level=100.0,\n            system_status=\"idle\",\n            current_task=\"none\",\n            last_command=\"\",\n            safety_status=\"safe\"\n        )\n\n        # Initialize subsystems\n        self.voice_system = VoiceCommandSystem(self)\n        self.navigation_system = NavigationSystem(self)\n        self.manipulation_system = ManipulationSystem(self)\n        self.perception_system = PerceptionSystem(self)\n        self.ai_system = AISystem(self)\n\n        # Initialize publishers\n        self.cmd_vel_pub = self.create_publisher(Twist, '/cmd_vel', 10)\n        self.joint_cmd_pub = self.create_publisher(\n            JointState, '/joint_group_position_controller/commands', 10\n        )\n        self.status_pub = self.create_publisher(String, '/humanoid/status', 10)\n        self.feedback_pub = self.create_publisher(String, '/humanoid/feedback', 10)\n\n        # Initialize subscribers\n        self.odom_sub = self.create_subscription(\n            Odometry, '/odom', self.odom_callback, 10\n        )\n        self.joint_state_sub = self.create_subscription(\n            JointState, '/joint_states', self.joint_state_callback, 10\n        )\n        self.laser_sub = self.create_subscription(\n            LaserScan, '/scan', self.laser_callback, 10\n        )\n        self.image_sub = self.create_subscription(\n            Image, '/camera/rgb/image_raw', self.image_callback, 10\n        )\n\n        # Initialize action servers\n        self.voice_command_server = ActionServer(\n            self,\n            VoiceCommandAction,\n            'voice_command',\n            self.execute_voice_command\n        )\n\n        # System parameters\n        self.system_params = {\n            'max_speed': 0.5,\n            'safety_distance': 0.5,\n            'battery_threshold': 20.0,\n            'response_timeout': 10.0,\n            'task_priority': ['emergency', 'navigation', 'manipulation', 'communication']\n        }\n\n        # Task queue for autonomous operation\n        self.task_queue = queue.Queue()\n        self.active_task = None\n\n        # System monitoring timer\n        self.monitor_timer = self.create_timer(1.0, self.system_monitor)\n\n        # Initialize all subsystems\n        self.initialize_subsystems()\n\n        self.get_logger().info('Autonomous Humanoid System initialized')\n\n    def initialize_subsystems(self):\n        \"\"\"Initialize all subsystems\"\"\"\n        try:\n            # Initialize voice command system\n            self.voice_system.initialize()\n\n            # Initialize navigation system\n            self.navigation_system.initialize()\n\n            # Initialize manipulation system\n            self.manipulation_system.initialize()\n\n            # Initialize perception system\n            self.perception_system.initialize()\n\n            # Initialize AI system\n            self.ai_system.initialize()\n\n            self.get_logger().info('All subsystems initialized successfully')\n\n        except Exception as e:\n            self.get_logger().error(f'Error initializing subsystems: {e}')\n\n    def odom_callback(self, msg: Odometry):\n        \"\"\"Update robot pose from odometry\"\"\"\n        self.robot_state.pose = msg.pose.pose\n\n    def joint_state_callback(self, msg: JointState):\n        \"\"\"Update joint states\"\"\"\n        self.robot_state.joint_states = list(msg.position)\n\n    def laser_callback(self, msg: LaserScan):\n        \"\"\"Process laser data for obstacle detection\"\"\"\n        # Check for obstacles\n        min_range = min(msg.ranges) if msg.ranges else float('inf')\n\n        if min_range < self.system_params['safety_distance']:\n            self.robot_state.safety_status = \"obstacle_detected\"\n        else:\n            self.robot_state.safety_status = \"safe\"\n\n    def image_callback(self, msg: Image):\n        \"\"\"Process camera image for perception\"\"\"\n        try:\n            cv_image = self.cv_bridge.imgmsg_to_cv2(msg, desired_encoding='bgr8')\n            # Process image through perception system\n            self.perception_system.process_image(cv_image)\n        except Exception as e:\n            self.get_logger().error(f'Error processing image: {e}')\n\n    def system_monitor(self):\n        \"\"\"Monitor overall system health and status\"\"\"\n        # Check battery level\n        if self.robot_state.battery_level < self.system_params['battery_threshold']:\n            self.get_logger().warn('Battery level low, consider returning to charging station')\n\n        # Check safety status\n        if self.robot_state.safety_status != \"safe\":\n            self.get_logger().warn(f'Safety issue detected: {self.robot_state.safety_status}')\n\n        # Publish system status\n        status_msg = String()\n        status_msg.data = json.dumps({\n            'system_status': self.robot_state.system_status,\n            'current_task': self.robot_state.current_task,\n            'battery_level': self.robot_state.battery_level,\n            'safety_status': self.robot_state.safety_status,\n            'pose': {\n                'x': self.robot_state.pose.position.x,\n                'y': self.robot_state.pose.position.y,\n                'z': self.robot_state.pose.position.z\n            }\n        })\n        self.status_pub.publish(status_msg)\n\n    def execute_voice_command(self, goal_handle):\n        \"\"\"Execute voice command action\"\"\"\n        command = goal_handle.request.command\n        self.get_logger().info(f'Executing voice command: {command}')\n\n        # Update robot state\n        self.robot_state.last_command = command\n        self.robot_state.current_task = \"voice_command_processing\"\n\n        try:\n            # Process command through AI system\n            action_plan = self.ai_system.process_command(command)\n\n            if action_plan:\n                # Execute the plan\n                success = self.execute_action_plan(action_plan)\n\n                if success:\n                    result = VoiceCommandResult()\n                    result.success = True\n                    result.message = f'Successfully executed: {command}'\n                    goal_handle.succeed()\n                else:\n                    result = VoiceCommandResult()\n                    result.success = False\n                    result.message = f'Failed to execute: {command}'\n                    goal_handle.abort()\n            else:\n                result = VoiceCommandResult()\n                result.success = False\n                result.message = f'Could not understand command: {command}'\n                goal_handle.abort()\n\n        except Exception as e:\n            result = VoiceCommandResult()\n            result.success = False\n            result.message = f'Error processing command: {e}'\n            goal_handle.abort()\n\n        return result\n\n    def execute_action_plan(self, action_plan: List[Dict[str, Any]]) -> bool:\n        \"\"\"Execute a plan of actions\"\"\"\n        try:\n            for action in action_plan:\n                action_type = action.get('type', 'unknown')\n                parameters = action.get('parameters', {})\n\n                success = False\n                if action_type == 'navigate':\n                    success = self.navigation_system.navigate_to(\n                        parameters.get('target_pose', Pose())\n                    )\n                elif action_type == 'manipulate':\n                    success = self.manipulation_system.manipulate_object(\n                        parameters.get('object_id', ''),\n                        parameters.get('action', 'grasp')\n                    )\n                elif action_type == 'speak':\n                    success = self.speak_text(parameters.get('text', ''))\n                elif action_type == 'wait':\n                    time.sleep(parameters.get('duration', 1.0))\n                    success = True\n                else:\n                    self.get_logger().warn(f'Unknown action type: {action_type}')\n                    continue\n\n                if not success:\n                    self.get_logger().error(f'Action failed: {action_type}')\n                    return False\n\n            return True\n\n        except Exception as e:\n            self.get_logger().error(f'Error executing action plan: {e}')\n            return False\n\n    def speak_text(self, text: str) -> bool:\n        \"\"\"Speak text using text-to-speech (simulated)\"\"\"\n        self.get_logger().info(f'Speaking: {text}')\n        # In real implementation, this would use TTS system\n        return True\n\nclass VoiceCommandSystem:\n    \"\"\"Voice command processing subsystem\"\"\"\n\n    def __init__(self, parent_node):\n        self.parent = parent_node\n        self.whisper_model = None\n        self.command_queue = queue.Queue()\n\n    def initialize(self):\n        \"\"\"Initialize voice command system\"\"\"\n        try:\n            # Load Whisper model\n            self.whisper_model = whisper.load_model('tiny', device='cpu')\n            self.parent.get_logger().info('Voice command system initialized')\n        except Exception as e:\n            self.parent.get_logger().error(f'Error initializing voice command system: {e}')\n\n    def process_audio(self, audio_data):\n        \"\"\"Process audio data for voice commands\"\"\"\n        if self.whisper_model is None:\n            return None\n\n        try:\n            # Convert audio to numpy array\n            audio_array = np.frombuffer(audio_data, dtype=np.int16)\n            audio_float = audio_array.astype(np.float32) / 32768.0\n\n            # Transcribe audio\n            result = self.whisper_model.transcribe(audio_float, language='en')\n            return result['text'].strip()\n\n        except Exception as e:\n            self.parent.get_logger().error(f'Error in audio processing: {e}')\n            return None\n\nclass NavigationSystem:\n    \"\"\"Navigation subsystem for humanoid robot\"\"\"\n\n    def __init__(self, parent_node):\n        self.parent = parent_node\n        self.nav_client = None  # Nav2 action client\n        self.current_goal = None\n\n    def initialize(self):\n        \"\"\"Initialize navigation system\"\"\"\n        # In real implementation, this would initialize Nav2 client\n        self.parent.get_logger().info('Navigation system initialized')\n\n    def navigate_to(self, target_pose: Pose) -> bool:\n        \"\"\"Navigate to target pose\"\"\"\n        try:\n            # In real implementation, this would send goal to Nav2\n            cmd_vel = Twist()\n            cmd_vel.linear.x = 0.2  # Move forward\n            self.parent.cmd_vel_pub.publish(cmd_vel)\n\n            # Simulate navigation completion\n            time.sleep(2.0)  # Simulated navigation time\n\n            # Stop robot\n            stop_cmd = Twist()\n            self.parent.cmd_vel_pub.publish(stop_cmd)\n\n            self.parent.get_logger().info(f'Navigated to target: ({target_pose.position.x}, {target_pose.position.y})')\n            return True\n\n        except Exception as e:\n            self.parent.get_logger().error(f'Error in navigation: {e}')\n            return False\n\nclass ManipulationSystem:\n    \"\"\"Manipulation subsystem for humanoid robot\"\"\"\n\n    def __init__(self, parent_node):\n        self.parent = parent_node\n        self.manipulation_client = None\n\n    def initialize(self):\n        \"\"\"Initialize manipulation system\"\"\"\n        self.parent.get_logger().info('Manipulation system initialized')\n\n    def manipulate_object(self, object_id: str, action: str) -> bool:\n        \"\"\"Manipulate an object\"\"\"\n        try:\n            # In real implementation, this would control robot arms/grippers\n            joint_cmd = JointState()\n            joint_cmd.position = self.parent.robot_state.joint_states.copy()\n\n            # Simulate manipulation action\n            if action == 'grasp':\n                # Adjust gripper position for grasping\n                if len(joint_cmd.position) > 18:  # Assuming gripper joints are at the end\n                    joint_cmd.position[-2] = 0.1  # Close gripper\n                    joint_cmd.position[-1] = 0.1\n            elif action == 'release':\n                # Open gripper\n                if len(joint_cmd.position) > 18:\n                    joint_cmd.position[-2] = 0.5  # Open gripper\n                    joint_cmd.position[-1] = 0.5\n\n            self.parent.joint_cmd_pub.publish(joint_cmd)\n            time.sleep(1.0)  # Simulated manipulation time\n\n            self.parent.get_logger().info(f'Manipulated object {object_id} with action {action}')\n            return True\n\n        except Exception as e:\n            self.parent.get_logger().error(f'Error in manipulation: {e}')\n            return False\n\nclass PerceptionSystem:\n    \"\"\"Perception subsystem for humanoid robot\"\"\"\n\n    def __init__(self, parent_node):\n        self.parent = parent_node\n        self.object_detector = None\n        self.scene_analyzer = None\n\n    def initialize(self):\n        \"\"\"Initialize perception system\"\"\"\n        # In real implementation, this would load perception models\n        self.parent.get_logger().info('Perception system initialized')\n\n    def process_image(self, image):\n        \"\"\"Process image for object detection and scene understanding\"\"\"\n        try:\n            # In real implementation, this would run object detection\n            # For simulation, we'll just log the image processing\n            height, width = image.shape[:2]\n            self.parent.get_logger().debug(f'Processed image: {width}x{height}')\n        except Exception as e:\n            self.parent.get_logger().error(f'Error in image processing: {e}')\n\nclass AISystem:\n    \"\"\"AI system for command understanding and planning\"\"\"\n\n    def __init__(self, parent_node):\n        self.parent = parent_node\n        self.llm_client = None  # OpenAI or local LLM client\n\n    def initialize(self):\n        \"\"\"Initialize AI system\"\"\"\n        # In real implementation, this would set up LLM client\n        self.parent.get_logger().info('AI system initialized')\n\n    def process_command(self, command: str) -> Optional[List[Dict[str, Any]]]:\n        \"\"\"Process natural language command and generate action plan\"\"\"\n        try:\n            # Simple command parsing - in real implementation, this would use LLM\n            command_lower = command.lower()\n\n            if 'move' in command_lower or 'go' in command_lower or 'navigate' in command_lower:\n                # Navigation command\n                target_location = self.extract_location(command)\n                return [{\n                    'type': 'navigate',\n                    'parameters': {\n                        'target_pose': self.get_location_pose(target_location)\n                    }\n                }]\n\n            elif 'pick' in command_lower or 'grasp' in command_lower or 'take' in command_lower:\n                # Manipulation command\n                object_id = self.extract_object(command)\n                return [{\n                    'type': 'manipulate',\n                    'parameters': {\n                        'object_id': object_id,\n                        'action': 'grasp'\n                    }\n                }]\n\n            elif 'speak' in command_lower or 'say' in command_lower:\n                # Communication command\n                text = self.extract_text(command)\n                return [{\n                    'type': 'speak',\n                    'parameters': {\n                        'text': text\n                    }\n                }]\n\n            elif 'stop' in command_lower or 'halt' in command_lower:\n                # Stop command\n                return [{\n                    'type': 'speak',\n                    'parameters': {\n                        'text': 'Stopping robot'\n                    }\n                }]\n\n            else:\n                # Unknown command - ask for clarification\n                return [{\n                    'type': 'speak',\n                    'parameters': {\n                        'text': f'I don\\'t understand the command: {command}. Please try again.'\n                    }\n                }]\n\n        except Exception as e:\n            self.parent.get_logger().error(f'Error processing command: {e}')\n            return None\n\n    def extract_location(self, command: str) -> str:\n        \"\"\"Extract location from command\"\"\"\n        # Simple location extraction\n        locations = ['kitchen', 'living room', 'bedroom', 'office', 'hallway']\n        for loc in locations:\n            if loc in command.lower():\n                return loc\n        return 'default_location'\n\n    def get_location_pose(self, location: str) -> Pose:\n        \"\"\"Get pose for a location\"\"\"\n        # In real implementation, this would look up location poses\n        pose = Pose()\n        if location == 'kitchen':\n            pose.position.x = 2.0\n            pose.position.y = 1.0\n        elif location == 'living room':\n            pose.position.x = 0.0\n            pose.position.y = 0.0\n        else:\n            pose.position.x = 1.0\n            pose.position.y = 1.0\n        return pose\n\n    def extract_object(self, command: str) -> str:\n        \"\"\"Extract object from command\"\"\"\n        objects = ['cup', 'bottle', 'book', 'box', 'chair']\n        for obj in objects:\n            if obj in command.lower():\n                return obj\n        return 'unknown_object'\n\n    def extract_text(self, command: str) -> str:\n        \"\"\"Extract text to speak from command\"\"\"\n        # Simple text extraction - in real implementation, use NLP\n        import re\n        # Look for text after \"say\" or \"speak\"\n        match = re.search(r'(?:say|speak)\\s+(.+)', command, re.IGNORECASE)\n        if match:\n            return match.group(1)\n        return \"Hello, I am your autonomous humanoid robot.\"\n\nclass VoiceCommandAction:\n    \"\"\"Simulated action definition for voice commands\"\"\"\n    pass\n\nclass VoiceCommandResult:\n    \"\"\"Simulated result for voice command action\"\"\"\n    def __init__(self):\n        self.success = False\n        self.message = \"\"\n\nclass AutonomousHumanoidManager:\n    \"\"\"High-level manager for autonomous humanoid operations\"\"\"\n\n    def __init__(self, node: AutonomousHumanoidNode):\n        self.node = node\n        self.active_behavior = None\n        self.behavior_queue = queue.Queue()\n\n    def start_autonomous_mode(self):\n        \"\"\"Start autonomous operation mode\"\"\"\n        self.node.get_logger().info('Starting autonomous mode')\n        self.node.robot_state.system_status = \"autonomous\"\n\n        # Start main behavior loop\n        import threading\n        self.behavior_thread = threading.Thread(target=self.behavior_loop, daemon=True)\n        self.behavior_thread.start()\n\n    def behavior_loop(self):\n        \"\"\"Main behavior execution loop\"\"\"\n        while rclpy.ok():\n            try:\n                # Check for new tasks\n                if not self.behavior_queue.empty():\n                    task = self.behavior_queue.get()\n                    self.execute_task(task)\n\n                # Perform autonomous behaviors\n                self.perform_autonomous_behaviors()\n\n                time.sleep(0.1)  # 10 Hz loop\n\n            except Exception as e:\n                self.node.get_logger().error(f'Error in behavior loop: {e}')\n                time.sleep(1.0)\n\n    def execute_task(self, task):\n        \"\"\"Execute a specific task\"\"\"\n        self.node.get_logger().info(f'Executing task: {task}')\n        # Implementation would depend on task type\n\n    def perform_autonomous_behaviors(self):\n        \"\"\"Perform autonomous behaviors like patrolling, monitoring, etc.\"\"\"\n        # Example autonomous behaviors:\n        # - Monitor battery level and return to charging when low\n        # - Patrol predefined areas\n        # - Monitor for human presence and interact appropriately\n        # - Perform routine maintenance tasks\n        pass\n\ndef main(args=None):\n    \"\"\"Main function for autonomous humanoid robot\"\"\"\n    rclpy.init(args=args)\n\n    # Create the main node\n    humanoid_node = AutonomousHumanoidNode()\n\n    # Create and start the manager\n    manager = AutonomousHumanoidManager(humanoid_node)\n    manager.start_autonomous_mode()\n\n    try:\n        # Run the node\n        rclpy.spin(humanoid_node)\n\n    except KeyboardInterrupt:\n        humanoid_node.get_logger().info('Shutting down autonomous humanoid system')\n    finally:\n        humanoid_node.destroy_node()\n        rclpy.shutdown()\n\n# Example launch file content for the complete system\n\"\"\"\n# humanoid_robot_launch.py\nfrom launch import LaunchDescription\nfrom launch_ros.actions import Node\nfrom launch.actions import DeclareLaunchArgument\nfrom launch.substitutions import LaunchConfiguration\n\ndef generate_launch_description():\n    return LaunchDescription([\n        # Autonomous humanoid main node\n        Node(\n            package='humanoid_robot',\n            executable='autonomous_humanoid',\n            name='autonomous_humanoid',\n            parameters=[\n                {'max_speed': 0.5},\n                {'safety_distance': 0.5},\n                {'battery_threshold': 20.0}\n            ],\n            output='screen'\n        ),\n\n        # Navigation system\n        Node(\n            package='nav2_bringup',\n            executable='nav2_launch.py',\n            name='navigation_system',\n            parameters=[\n                {'use_sim_time': False}\n            ]\n        ),\n\n        # Perception system\n        Node(\n            package='perception_pkg',\n            executable='object_detection',\n            name='object_detection',\n            parameters=[\n                {'detection_model': 'yolov8'}\n            ]\n        ),\n\n        # Voice processing\n        Node(\n            package='voice_pkg',\n            executable='voice_processor',\n            name='voice_processor'\n        )\n    ])\n\"\"\"\n\n# Example configuration file for the complete system\n\"\"\"\n# humanoid_config.yaml\nautonomous_humanoid:\n  ros__parameters:\n    max_speed: 0.5\n    safety_distance: 0.5\n    battery_threshold: 20.0\n    response_timeout: 10.0\n    task_priority: ['emergency', 'navigation', 'manipulation', 'communication']\n\n    voice_system:\n      model: 'tiny'\n      language: 'en'\n      confidence_threshold: 0.7\n\n    navigation_system:\n      planner: 'navfn'\n      controller: 'dwb'\n      recovery_enabled: true\n\n    manipulation_system:\n      gripper_joints: ['left_gripper_finger1_joint', 'left_gripper_finger2_joint']\n      max_force: 50.0\n\"\"\"\n"})}),"\n",(0,i.jsx)(e.h2,{id:"diagrams",children:"Diagrams"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{children:"Autonomous Humanoid System Architecture:\n\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                    Autonomous Humanoid System                   \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\n\u2502  \u2502  Voice      \u2502  \u2502  Perception \u2502  \u2502  Navigation \u2502  \u2502  Manip. \u2502 \u2502\n\u2502  \u2502  Command    \u2502  \u2502  System     \u2502  \u2502  System     \u2502  \u2502  System \u2502 \u2502\n\u2502  \u2502  Processing \u2502  \u2502  (Vision,   \u2502  \u2502  (Path      \u2502  \u2502  (Arm   \u2502 \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502   Audio)    \u2502  \u2502   Planning) \u2502  \u2502   Control) \u2502\n\u2502         \u2502         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\n\u2502         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518      \u2502\n\u2502                           \u2502                 \u2502                    \u2502\n\u2502         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\n\u2502         \u2502                AI Cognitive                       \u2502  \u2502\n\u2502         \u2502              Planning System                      \u2502  \u2502\n\u2502         \u2502           (LLM, Task Planning)                    \u2502  \u2502\n\u2502         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n\u2502                           \u2502                                    \u2502\n\u2502         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\n\u2502         \u2502              Action Execution                   \u2502  \u2502\n\u2502         \u2502           (ROS 2 Actions)                       \u2502  \u2502\n\u2502         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n\u2502                           \u2502                                    \u2502\n\u2502         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\n\u2502         \u2502              Robot Hardware                     \u2502  \u2502\n\u2502         \u2502         (Motors, Sensors, etc.)                 \u2502  \u2502\n\u2502         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\nSystem Integration Flow:\n\nUser Voice \u2500\u2500\u25ba Speech Recognition \u2500\u2500\u25ba NLU \u2500\u2500\u25ba AI Planning \u2500\u2500\u25ba Action Execution\nCommand        (Whisper)           (Intent)  (LLM)         (ROS Actions)\n                 \u2502                   \u2502        \u2502              \u2502\n                 \u25bc                   \u25bc        \u25bc              \u25bc\n            Audio Processing    Command    Task        Robot\n            (Noise Reduction)   Parsing   Planning     Control\n\nReal-time Operation Loop:\n\nPerception \u2500\u2500\u25ba Planning \u2500\u2500\u25ba Execution \u2500\u2500\u25ba Monitoring \u2500\u2500\u25ba Adjustment\n(0.1s)      (0.2s)      (0.1s)       (0.05s)       (0.05s)\n    \u2502         \u2502           \u2502            \u2502             \u2502\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                 (10Hz operation)\n"})}),"\n",(0,i.jsx)(e.h2,{id:"case-study",children:"Case Study"}),"\n",(0,i.jsx)(e.p,{children:"The integration of all the concepts from this book can be seen in advanced humanoid robots like Boston Dynamics' Atlas or SoftBank's Pepper. These robots demonstrate the power of integrated systems where perception, cognition, and action work together seamlessly. Atlas combines sophisticated perception systems with advanced control algorithms to perform complex tasks like running, jumping, and manipulating objects. The key to their success lies in the tight integration of all subsystems, similar to what we've built in this capstone project."}),"\n",(0,i.jsx)(e.h2,{id:"references",children:"References"}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:(0,i.jsx)(e.a,{href:"https://docs.ros.org/en/rolling/",children:"ROS 2 Documentation"})}),"\n",(0,i.jsx)(e.li,{children:(0,i.jsx)(e.a,{href:"https://navigation.ros.org/",children:"Navigation2 Documentation"})}),"\n",(0,i.jsx)(e.li,{children:(0,i.jsx)(e.a,{href:"https://platform.openai.com/docs/",children:"OpenAI API Documentation"})}),"\n",(0,i.jsx)(e.li,{children:(0,i.jsx)(e.a,{href:"https://github.com/openai/whisper",children:"Whisper Speech Recognition"})}),"\n"]}),"\n",(0,i.jsx)(e.h2,{id:"review-questions",children:"Review Questions"}),"\n",(0,i.jsxs)(e.ol,{children:["\n",(0,i.jsx)(e.li,{children:"How do the different subsystems in the autonomous humanoid work together?"}),"\n",(0,i.jsx)(e.li,{children:"What are the key challenges in integrating all the components?"}),"\n",(0,i.jsx)(e.li,{children:"How does the system handle real-time performance requirements?"}),"\n"]}),"\n",(0,i.jsx)(e.h2,{id:"practical-exercises",children:"Practical Exercises"}),"\n",(0,i.jsxs)(e.ol,{children:["\n",(0,i.jsx)(e.li,{children:"Implement the complete autonomous humanoid system"}),"\n",(0,i.jsx)(e.li,{children:"Test the system with various voice commands"}),"\n",(0,i.jsx)(e.li,{children:"Validate navigation and manipulation capabilities"}),"\n",(0,i.jsx)(e.li,{children:"Measure system performance and response times"}),"\n"]})]})}function d(n={}){const{wrapper:e}={...(0,s.R)(),...n.components};return e?(0,i.jsx)(e,{...n,children:(0,i.jsx)(m,{...n})}):m(n)}},8453:(n,e,t)=>{t.d(e,{R:()=>a,x:()=>r});var o=t(6540);const i={},s=o.createContext(i);function a(n){const e=o.useContext(s);return o.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function r(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(i):n.components||i:a(n.components),o.createElement(s.Provider,{value:e},n.children)}}}]);