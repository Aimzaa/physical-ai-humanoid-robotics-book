"use strict";(globalThis.webpackChunkphysical_ai_humanoid_robotics_book=globalThis.webpackChunkphysical_ai_humanoid_robotics_book||[]).push([[499],{4787:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>s,default:()=>m,frontMatter:()=>r,metadata:()=>i,toc:()=>c});const i=JSON.parse('{"id":"capstone-project/complete-voice-to-action","title":"Complete Voice-to-Action System Implementation","description":"Goal","source":"@site/docs/capstone-project/complete-voice-to-action.md","sourceDirName":"capstone-project","slug":"/capstone-project/complete-voice-to-action","permalink":"/physical-ai-humanoid-robotics-book/docs/capstone-project/complete-voice-to-action","draft":false,"unlisted":false,"editUrl":"https://github.com/your-org/physical-ai-humanoid-robotics-book/tree/main/docs/capstone-project/complete-voice-to-action.md","tags":[],"version":"current","frontMatter":{"id":"complete-voice-to-action","title":"Complete Voice-to-Action System Implementation","sidebar_label":"Complete Voice-to-Action System"},"sidebar":"book","previous":{"title":"Integration of All Module Concepts","permalink":"/physical-ai-humanoid-robotics-book/docs/capstone-project/integration-concepts"},"next":{"title":"Integrated System Diagram","permalink":"/physical-ai-humanoid-robotics-book/docs/capstone-project/integrated-system-diagram"}}');var o=t(4848),a=t(8453);const r={id:"complete-voice-to-action",title:"Complete Voice-to-Action System Implementation",sidebar_label:"Complete Voice-to-Action System"},s="Complete Voice-to-Action System Implementation",l={},c=[{value:"Goal",id:"goal",level:2},{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Overview",id:"overview",level:2},{value:"Key Concepts",id:"key-concepts",level:2},{value:"Step-by-Step Breakdown",id:"step-by-step-breakdown",level:2},{value:"Code Examples",id:"code-examples",level:2}];function p(e){const n={code:"code",h1:"h1",h2:"h2",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,a.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.header,{children:(0,o.jsx)(n.h1,{id:"complete-voice-to-action-system-implementation",children:"Complete Voice-to-Action System Implementation"})}),"\n",(0,o.jsx)(n.h2,{id:"goal",children:"Goal"}),"\n",(0,o.jsx)(n.p,{children:"Implement a complete end-to-end voice-to-action system that integrates speech recognition, natural language understanding, AI planning, and robot execution into a seamless pipeline for humanoid robot control."}),"\n",(0,o.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Understand the complete voice-to-action pipeline architecture"}),"\n",(0,o.jsx)(n.li,{children:"Implement real-time speech recognition and processing"}),"\n",(0,o.jsx)(n.li,{children:"Integrate natural language understanding with AI planning"}),"\n",(0,o.jsx)(n.li,{children:"Connect voice commands to robot action execution"}),"\n",(0,o.jsx)(n.li,{children:"Optimize the pipeline for real-time performance"}),"\n",(0,o.jsx)(n.li,{children:"Validate system accuracy and response time"}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,o.jsx)(n.p,{children:"The complete voice-to-action system represents the culmination of the VLA (Vision-Language-Action) module concepts, creating an end-to-end pipeline that allows users to control humanoid robots using natural language commands. This system integrates automatic speech recognition (ASR), natural language understanding (NLU), large language model (LLM) planning, and action execution into a unified pipeline that provides natural human-robot interaction. The system must handle real-time processing requirements while maintaining accuracy and safety."}),"\n",(0,o.jsx)(n.h2,{id:"key-concepts",children:"Key Concepts"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"End-to-End Pipeline"}),": Complete flow from voice input to robot action"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Real-Time Processing"}),": Meeting timing constraints for responsive interaction"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Natural Language Understanding"}),": Interpreting user intent from speech"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"AI Planning"}),": Generating action sequences from commands"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Action Execution"}),": Executing planned actions on the robot"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Pipeline Optimization"}),": Techniques to improve performance"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Error Handling"}),": Managing failures gracefully"]}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"step-by-step-breakdown",children:"Step-by-Step Breakdown"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsx)(n.p,{children:(0,o.jsx)(n.strong,{children:"Pipeline Architecture Design"})}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Design complete voice-to-action system architecture"}),"\n",(0,o.jsx)(n.li,{children:"Plan component interfaces and data flow"}),"\n",(0,o.jsx)(n.li,{children:"Configure real-time processing requirements"}),"\n",(0,o.jsx)(n.li,{children:"Establish safety and error handling protocols"}),"\n"]}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsx)(n.p,{children:(0,o.jsx)(n.strong,{children:"Speech Recognition Integration"})}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Integrate Whisper or similar ASR system"}),"\n",(0,o.jsx)(n.li,{children:"Configure audio preprocessing and noise reduction"}),"\n",(0,o.jsx)(n.li,{children:"Optimize for real-time performance"}),"\n",(0,o.jsx)(n.li,{children:"Handle different acoustic environments"}),"\n"]}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsx)(n.p,{children:(0,o.jsx)(n.strong,{children:"Natural Language Understanding"})}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Implement intent classification and entity extraction"}),"\n",(0,o.jsx)(n.li,{children:"Connect to language understanding models"}),"\n",(0,o.jsx)(n.li,{children:"Handle ambiguous or unclear commands"}),"\n",(0,o.jsx)(n.li,{children:"Create command-to-action mappings"}),"\n"]}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsx)(n.p,{children:(0,o.jsx)(n.strong,{children:"AI Planning System"})}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Integrate LLM-based planning capabilities"}),"\n",(0,o.jsx)(n.li,{children:"Create task decomposition from commands"}),"\n",(0,o.jsx)(n.li,{children:"Handle multi-step action sequences"}),"\n",(0,o.jsx)(n.li,{children:"Implement plan validation and safety checks"}),"\n"]}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsx)(n.p,{children:(0,o.jsx)(n.strong,{children:"Action Execution Pipeline"})}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Connect to robot action execution systems"}),"\n",(0,o.jsx)(n.li,{children:"Implement action sequencing and monitoring"}),"\n",(0,o.jsx)(n.li,{children:"Handle action failures and recovery"}),"\n",(0,o.jsx)(n.li,{children:"Ensure safe robot operation"}),"\n"]}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsx)(n.p,{children:(0,o.jsx)(n.strong,{children:"System Validation and Optimization"})}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Test pipeline performance and accuracy"}),"\n",(0,o.jsx)(n.li,{children:"Optimize for real-time response"}),"\n",(0,o.jsx)(n.li,{children:"Validate safety and reliability"}),"\n",(0,o.jsx)(n.li,{children:"Document performance metrics"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"code-examples",children:"Code Examples"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"# Complete voice-to-action system implementation\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String, Bool, Float32MultiArray\nfrom sensor_msgs.msg import AudioData, Image\nfrom geometry_msgs.msg import Twist, Pose\nfrom rclpy.action import ActionServer, ActionClient\nfrom rclpy.qos import QoSProfile, HistoryPolicy, ReliabilityPolicy\nimport numpy as np\nimport torch\nimport whisper\nimport speech_recognition as sr\nimport threading\nimport queue\nimport time\nimport json\nfrom typing import Dict, List, Optional, Tuple, Any\nfrom dataclasses import dataclass\nimport openai\nimport asyncio\nfrom concurrent.futures import ThreadPoolExecutor\nimport cv2\nfrom cv_bridge import CvBridge\n\n@dataclass\nclass VoiceCommand:\n    \"\"\"Represents a voice command in the pipeline\"\"\"\n    audio_data: Optional[bytes] = None\n    text: str = \"\"\n    confidence: float = 0.0\n    timestamp: float = 0.0\n    user_id: str = \"\"\n    processed: bool = False\n\n@dataclass\nclass PipelineStage:\n    \"\"\"Represents a stage in the voice-to-action pipeline\"\"\"\n    name: str\n    start_time: float = 0.0\n    end_time: float = 0.0\n    success: bool = False\n    result: Optional[Any] = None\n    error: Optional[str] = None\n\n@dataclass\nclass ActionPlan:\n    \"\"\"Represents an action plan generated from voice command\"\"\"\n    command: str\n    intent: str\n    entities: List[str]\n    actions: List[Dict[str, Any]]\n    timestamp: float\n    confidence: float\n\nclass VoiceToActionPipeline(Node):\n    \"\"\"Complete voice-to-action pipeline system\"\"\"\n\n    def __init__(self):\n        super().__init__('voice_to_action_pipeline')\n\n        # Initialize CV bridge for image processing\n        self.cv_bridge = CvBridge()\n\n        # Initialize pipeline state\n        self.pipeline_state = {\n            'current_stage': 'idle',\n            'active_command': None,\n            'pipeline_active': False,\n            'error_count': 0,\n            'success_count': 0,\n            'average_latency': 0.0\n        }\n\n        # Initialize queues for pipeline stages\n        self.audio_queue = queue.Queue(maxsize=20)\n        self.recognition_queue = queue.Queue(maxsize=20)\n        self.nlu_queue = queue.Queue(maxsize=20)\n        self.planning_queue = queue.Queue(maxsize=20)\n        self.execution_queue = queue.Queue(maxsize=20)\n\n        # Initialize Whisper model\n        self.whisper_model = None\n        self.load_whisper_model()\n\n        # Initialize OpenAI client for planning (use API key securely in real implementation)\n        # openai.api_key = \"your-api-key-here\"\n\n        # Initialize audio processing parameters\n        self.audio_params = {\n            'sample_rate': 16000,\n            'chunk_size': 1024,\n            'channels': 1,\n            'format': 'int16',\n            'vad_threshold': 0.01,  # Voice activity detection threshold\n            'silence_duration': 0.5  # Seconds of silence to trigger processing\n        }\n\n        # Initialize pipeline parameters\n        self.pipeline_params = {\n            'max_command_length': 100,\n            'confidence_threshold': 0.7,\n            'command_timeout': 10.0,\n            'pipeline_retry_limit': 3,\n            'response_timeout': 5.0,\n            'wake_word': 'robot',\n            'enable_confirmation': True\n        }\n\n        # Initialize publishers\n        self.command_pub = self.create_publisher(String, '/voice/command', 10)\n        self.action_pub = self.create_publisher(String, '/robot/action', 10)\n        self.status_pub = self.create_publisher(String, '/voice_pipeline/status', 10)\n        self.feedback_pub = self.create_publisher(String, '/voice_pipeline/feedback', 10)\n        self.response_pub = self.create_publisher(String, '/voice_pipeline/response', 10)\n\n        # Initialize robot control publishers\n        self.cmd_vel_pub = self.create_publisher(Twist, '/cmd_vel', 10)\n        self.joint_cmd_pub = self.create_publisher(\n            Float32MultiArray, '/joint_group_position_controller/commands', 10\n        )\n\n        # Initialize subscribers\n        self.audio_sub = self.create_subscription(\n            AudioData, '/audio/data', self.audio_callback, 10\n        )\n        self.image_sub = self.create_subscription(\n            Image, '/camera/rgb/image_raw', self.image_callback, 10\n        )\n\n        # Initialize action servers\n        self.voice_action_server = ActionServer(\n            self,\n            VoiceCommandAction,\n            'voice_command',\n            self.execute_voice_command\n        )\n\n        # Initialize pipeline processing threads\n        self.pipeline_executor = ThreadPoolExecutor(max_workers=6)\n\n        self.audio_thread = threading.Thread(target=self.process_audio, daemon=True)\n        self.recognition_thread = threading.Thread(target=self.process_recognition, daemon=True)\n        self.nlu_thread = threading.Thread(target=self.process_nlu, daemon=True)\n        self.planning_thread = threading.Thread(target=self.process_planning, daemon=True)\n        self.execution_thread = threading.Thread(target=self.process_execution, daemon=True)\n\n        self.audio_thread.start()\n        self.recognition_thread.start()\n        self.nlu_thread.start()\n        self.planning_thread.start()\n        self.execution_thread.start()\n\n        # Initialize performance tracking\n        self.latency_measurements = []\n        self.performance_timer = self.create_timer(1.0, self.performance_monitor)\n\n        self.get_logger().info('Complete Voice-to-Action Pipeline initialized')\n\n    def load_whisper_model(self):\n        \"\"\"Load Whisper model for speech recognition\"\"\"\n        try:\n            # Load a model suitable for real-time processing\n            self.whisper_model = whisper.load_model('tiny', device='cpu')\n            self.get_logger().info('Whisper model loaded successfully')\n        except Exception as e:\n            self.get_logger().error(f'Error loading Whisper model: {e}')\n            # Fallback to speech_recognition library\n            self.speech_recognizer = sr.Recognizer()\n\n    def audio_callback(self, msg: AudioData):\n        \"\"\"Handle incoming audio data\"\"\"\n        try:\n            # Add audio data to processing queue\n            audio_item = {\n                'data': msg.data,\n                'timestamp': time.time(),\n                'header': msg.header\n            }\n\n            try:\n                self.audio_queue.put_nowait(audio_item)\n                self.pipeline_state['current_stage'] = 'audio_processing'\n            except queue.Full:\n                self.get_logger().warn('Audio queue full, dropping frame')\n\n        except Exception as e:\n            self.get_logger().error(f'Error in audio callback: {e}')\n\n    def image_callback(self, msg: Image):\n        \"\"\"Process image for context-aware voice commands\"\"\"\n        try:\n            # Convert image for context understanding\n            cv_image = self.cv_bridge.imgmsg_to_cv2(msg, desired_encoding='bgr8')\n\n            # In real implementation, this would provide context for voice commands\n            # For example, object detection to understand \"pick up the red cup\"\n            # that's visible in the camera\n            pass\n\n        except Exception as e:\n            self.get_logger().error(f'Error processing image for context: {e}')\n\n    def process_audio(self):\n        \"\"\"Process audio data and perform speech recognition\"\"\"\n        audio_buffer = []\n        silence_start_time = None\n        speech_detected = False\n\n        while rclpy.ok():\n            try:\n                # Get audio data from queue\n                audio_item = self.audio_queue.get(timeout=0.1)\n                audio_data = audio_item['data']\n\n                # Convert audio data to numpy array\n                audio_array = np.frombuffer(audio_data, dtype=np.int16)\n\n                # Calculate audio energy for VAD (Voice Activity Detection)\n                energy = np.mean(np.abs(audio_array.astype(np.float32)))\n\n                if energy > self.audio_params['vad_threshold']:\n                    # Speech detected, add to buffer\n                    audio_buffer.extend(audio_array)\n                    speech_detected = True\n                    silence_start_time = None\n                else:\n                    # Silence detected\n                    if speech_detected:\n                        if silence_start_time is None:\n                            silence_start_time = time.time()\n                        elif time.time() - silence_start_time > self.audio_params['silence_duration']:\n                            # Process the buffered audio\n                            if len(audio_buffer) > 0:\n                                self.process_speech_buffer(audio_buffer)\n                                audio_buffer = []\n                                speech_detected = False\n                                silence_start_time = None\n                    else:\n                        # Continue accumulating silence (for longer speech segments)\n                        audio_buffer.extend(audio_array)\n\n            except queue.Empty:\n                continue\n            except Exception as e:\n                self.get_logger().error(f'Error in audio processing: {e}')\n\n    def process_speech_buffer(self, audio_buffer):\n        \"\"\"Process accumulated speech buffer\"\"\"\n        try:\n            # Convert buffer to numpy array\n            audio_array = np.array(audio_buffer, dtype=np.int16)\n            audio_float = audio_array.astype(np.float32) / 32768.0\n\n            # Perform speech recognition\n            if self.whisper_model is not None:\n                start_time = time.time()\n                result = self.whisper_model.transcribe(\n                    audio_float,\n                    language='en',\n                    task='transcribe'\n                )\n                processing_time = time.time() - start_time\n\n                text = result['text'].strip()\n                confidence = result.get('avg_logprob', -1.0)\n\n                if text and confidence > self.pipeline_params['confidence_threshold']:\n                    # Create voice command\n                    voice_cmd = VoiceCommand(\n                        text=text,\n                        confidence=confidence,\n                        timestamp=time.time()\n                    )\n\n                    # Add to recognition queue\n                    try:\n                        self.recognition_queue.put_nowait(voice_cmd)\n                        self.get_logger().info(f'Recognized: \"{text}\" (conf: {confidence:.2f})')\n                    except queue.Full:\n                        self.get_logger().warn('Recognition queue full')\n                else:\n                    self.get_logger().debug(f'Low confidence recognition: {confidence:.2f}')\n\n        except Exception as e:\n            self.get_logger().error(f'Error processing speech buffer: {e}')\n\n    def process_recognition(self):\n        \"\"\"Process recognized text through NLU\"\"\"\n        while rclpy.ok():\n            try:\n                voice_cmd = self.recognition_queue.get(timeout=0.1)\n\n                # Update pipeline state\n                self.pipeline_state['current_stage'] = 'nlu_processing'\n                self.pipeline_state['active_command'] = voice_cmd\n\n                # Check for wake word if configured\n                if self.pipeline_params['wake_word']:\n                    if self.pipeline_params['wake_word'].lower() in voice_cmd.text.lower():\n                        # Extract command after wake word\n                        command_text = voice_cmd.text.lower().replace(\n                            self.pipeline_params['wake_word'].lower(), ''\n                        ).strip()\n                        voice_cmd.text = command_text\n                    else:\n                        # No wake word detected, skip processing\n                        continue\n\n                # Perform natural language understanding\n                nlu_result = self.perform_nlu(voice_cmd.text)\n\n                if nlu_result:\n                    # Add to NLU queue\n                    nlu_item = {\n                        'command': voice_cmd,\n                        'nlu_result': nlu_result,\n                        'timestamp': time.time()\n                    }\n\n                    try:\n                        self.nlu_queue.put_nowait(nlu_item)\n                        self.get_logger().debug(f'NLU completed for: {voice_cmd.text}')\n                    except queue.Full:\n                        self.get_logger().warn('NLU queue full')\n\n            except queue.Empty:\n                continue\n            except Exception as e:\n                self.get_logger().error(f'Error in recognition processing: {e}')\n                self.handle_pipeline_error(e)\n\n    def perform_nlu(self, text: str) -> Optional[Dict[str, Any]]:\n        \"\"\"Perform natural language understanding on text\"\"\"\n        try:\n            # Classify intent\n            intent = self.classify_intent(text)\n\n            # Extract entities\n            entities = self.extract_entities(text)\n\n            # Determine action type\n            action_type = self.determine_action_type(text)\n\n            # Extract parameters\n            parameters = self.extract_parameters(text)\n\n            nlu_result = {\n                'intent': intent,\n                'entities': entities,\n                'action_type': action_type,\n                'parameters': parameters,\n                'original_text': text,\n                'timestamp': time.time()\n            }\n\n            return nlu_result\n\n        except Exception as e:\n            self.get_logger().error(f'Error in NLU: {e}')\n            return None\n\n    def classify_intent(self, text: str) -> str:\n        \"\"\"Classify the intent of the command\"\"\"\n        text_lower = text.lower()\n\n        # Navigation intents\n        navigation_keywords = ['move', 'go', 'walk', 'navigate', 'drive', 'turn', 'forward', 'backward', 'left', 'right']\n        if any(keyword in text_lower for keyword in navigation_keywords):\n            return 'navigation'\n\n        # Manipulation intents\n        manipulation_keywords = ['pick', 'grasp', 'take', 'grab', 'lift', 'place', 'put', 'release', 'hold']\n        if any(keyword in text_lower for keyword in manipulation_keywords):\n            return 'manipulation'\n\n        # Communication intents\n        communication_keywords = ['speak', 'say', 'tell', 'hello', 'hi', 'goodbye', 'bye', 'talk']\n        if any(keyword in text_lower for keyword in communication_keywords):\n            return 'communication'\n\n        # Stop/interrupt intents\n        stop_keywords = ['stop', 'halt', 'pause', 'wait', 'cancel']\n        if any(keyword in text_lower for keyword in stop_keywords):\n            return 'stop'\n\n        # Default to unknown\n        return 'unknown'\n\n    def extract_entities(self, text: str) -> List[str]:\n        \"\"\"Extract entities from text\"\"\"\n        entities = []\n        text_lower = text.lower()\n\n        # Common objects\n        common_objects = ['cup', 'bottle', 'book', 'chair', 'table', 'person', 'door', 'window', 'box', 'ball']\n        for obj in common_objects:\n            if obj in text_lower:\n                entities.append(obj)\n\n        # Common locations\n        common_locations = ['kitchen', 'living room', 'bedroom', 'office', 'hallway', 'bathroom', 'garden']\n        for loc in common_locations:\n            if loc in text_lower:\n                entities.append(loc)\n\n        # Colors\n        colors = ['red', 'blue', 'green', 'yellow', 'black', 'white', 'orange', 'purple']\n        for color in colors:\n            if color in text_lower:\n                entities.append(color)\n\n        return entities\n\n    def determine_action_type(self, text: str) -> str:\n        \"\"\"Determine the action type from text\"\"\"\n        text_lower = text.lower()\n\n        if any(word in text_lower for word in ['move', 'go', 'walk', 'navigate']):\n            return 'move_to_location'\n        elif any(word in text_lower for word in ['pick', 'grasp', 'take', 'grab']):\n            return 'grasp_object'\n        elif any(word in text_lower for word in ['speak', 'say', 'tell']):\n            return 'speak_text'\n        elif any(word in text_lower for word in ['stop', 'halt', 'pause']):\n            return 'stop_robot'\n        elif any(word in text_lower for word in ['dance', 'wave', 'greet']):\n            return 'perform_action'\n        else:\n            return 'unknown'\n\n    def extract_parameters(self, text: str) -> Dict[str, Any]:\n        \"\"\"Extract parameters from text\"\"\"\n        params = {}\n\n        # Extract numeric values\n        import re\n        numbers = re.findall(r'\\d+\\.?\\d*', text)\n        if numbers:\n            params['numeric_values'] = [float(n) for n in numbers if n]\n\n        # Extract direction words\n        directions = ['forward', 'backward', 'left', 'right', 'up', 'down']\n        found_directions = [d for d in directions if d in text.lower()]\n        if found_directions:\n            params['direction'] = found_directions[0]\n\n        # Extract duration\n        duration_match = re.search(r'(\\d+)\\s*(seconds?|secs?|minutes?|mins?)', text.lower())\n        if duration_match:\n            value, unit = duration_match.groups()\n            multiplier = 60 if 'minute' in unit else 1\n            params['duration'] = float(value) * multiplier\n\n        return params\n\n    def process_nlu(self):\n        \"\"\"Process NLU results through cognitive planning\"\"\"\n        while rclpy.ok():\n            try:\n                nlu_item = self.nlu_queue.get(timeout=0.1)\n\n                # Update pipeline state\n                self.pipeline_state['current_stage'] = 'planning'\n\n                # Generate plan using AI system\n                plan = self.generate_plan(nlu_item['nlu_result'], nlu_item['command'].text)\n\n                if plan:\n                    # Add to planning queue\n                    planning_item = {\n                        'command': nlu_item['command'],\n                        'nlu_result': nlu_item['nlu_result'],\n                        'plan': plan,\n                        'timestamp': time.time()\n                    }\n\n                    try:\n                        self.planning_queue.put_nowait(planning_item)\n                        self.get_logger().debug(f'Plan generated for: {nlu_item[\"command\"].text}')\n                    except queue.Full:\n                        self.get_logger().warn('Planning queue full')\n\n            except queue.Empty:\n                continue\n            except Exception as e:\n                self.get_logger().error(f'Error in NLU processing: {e}')\n                self.handle_pipeline_error(e)\n\n    def generate_plan(self, nlu_result: Dict[str, Any], command_text: str) -> Optional[ActionPlan]:\n        \"\"\"Generate action plan based on NLU result\"\"\"\n        try:\n            intent = nlu_result.get('intent', 'unknown')\n            entities = nlu_result.get('entities', [])\n            action_type = nlu_result.get('action_type', 'unknown')\n            parameters = nlu_result.get('parameters', {})\n\n            # Generate action sequence based on intent\n            actions = []\n\n            if intent == 'navigation':\n                # Navigation plan\n                target_location = entities[0] if entities else 'destination'\n                actions = [\n                    {'type': 'speak', 'parameters': {'text': f'Navigating to {target_location}'}},\n                    {'type': 'navigate_to', 'parameters': {'location': target_location}},\n                    {'type': 'confirm_arrival', 'parameters': {'location': target_location}}\n                ]\n            elif intent == 'manipulation':\n                # Manipulation plan\n                target_object = entities[0] if entities else 'object'\n                actions = [\n                    {'type': 'speak', 'parameters': {'text': f'Looking for {target_object}'}},\n                    {'type': 'locate_object', 'parameters': {'object': target_object}},\n                    {'type': 'approach_object', 'parameters': {'object': target_object}},\n                    {'type': 'grasp_object', 'parameters': {'object': target_object}},\n                    {'type': 'confirm_grasp', 'parameters': {}}\n                ]\n            elif intent == 'communication':\n                # Communication plan\n                actions = [\n                    {'type': 'speak', 'parameters': {'text': f'You said: {command_text}'}},\n                    {'type': 'wait_for_response', 'parameters': {'duration': 2.0}}\n                ]\n            elif intent == 'stop':\n                # Stop plan\n                actions = [\n                    {'type': 'stop_robot', 'parameters': {}},\n                    {'type': 'speak', 'parameters': {'text': 'Robot stopped'}}\n                ]\n            elif intent == 'perform_action':\n                # Action performance plan\n                action_name = command_text.split()[0] if command_text.split() else 'action'\n                actions = [\n                    {'type': 'speak', 'parameters': {'text': f'Performing {action_name}'}},\n                    {'type': 'perform_action', 'parameters': {'action': action_name}},\n                    {'type': 'confirm_completion', 'parameters': {}}\n                ]\n            else:\n                # Unknown intent - ask for clarification\n                actions = [\n                    {'type': 'speak', 'parameters': {'text': f'I don\\'t understand: {command_text}. Please try again.'}},\n                    {'type': 'request_clarification', 'parameters': {'command': command_text}}\n                ]\n\n            # Create action plan\n            action_plan = ActionPlan(\n                command=command_text,\n                intent=intent,\n                entities=entities,\n                actions=actions,\n                timestamp=time.time(),\n                confidence=0.8  # Default confidence\n            )\n\n            return action_plan\n\n        except Exception as e:\n            self.get_logger().error(f'Error in plan generation: {e}')\n            return None\n\n    def process_planning(self):\n        \"\"\"Process plans through action execution\"\"\"\n        while rclpy.ok():\n            try:\n                planning_item = self.planning_queue.get(timeout=0.1)\n\n                # Update pipeline state\n                self.pipeline_state['current_stage'] = 'execution'\n\n                # Execute the plan\n                execution_success = self.execute_plan(planning_item['plan'])\n\n                if execution_success:\n                    # Add to execution queue for final processing\n                    execution_item = {\n                        'command': planning_item['command'],\n                        'plan': planning_item['plan'],\n                        'success': True,\n                        'timestamp': time.time()\n                    }\n\n                    try:\n                        self.execution_queue.put_nowait(execution_item)\n                        self.get_logger().info(f'Plan executed successfully: {planning_item[\"command\"].text}')\n                    except queue.Full:\n                        self.get_logger().warn('Execution queue full')\n\n            except queue.Empty:\n                continue\n            except Exception as e:\n                self.get_logger().error(f'Error in planning processing: {e}')\n                self.handle_pipeline_error(e)\n\n    def execute_plan(self, plan: ActionPlan) -> bool:\n        \"\"\"Execute an action plan\"\"\"\n        try:\n            for i, action in enumerate(plan.actions):\n                action_type = action['type']\n                parameters = action['parameters']\n\n                self.get_logger().info(f'Executing action {i+1}/{len(plan.actions)}: {action_type}')\n\n                # Execute action\n                success = self.execute_single_action(action_type, parameters)\n\n                if not success:\n                    self.get_logger().error(f'Action failed: {action_type}')\n                    return False\n\n                # Small delay between actions for safety\n                time.sleep(0.1)\n\n            return True\n\n        except Exception as e:\n            self.get_logger().error(f'Error in plan execution: {e}')\n            return False\n\n    def execute_single_action(self, action_type: str, parameters: Dict[str, Any]) -> bool:\n        \"\"\"Execute a single action\"\"\"\n        try:\n            if action_type == 'speak':\n                text = parameters.get('text', '')\n                return self.speak_text(text)\n            elif action_type == 'navigate_to':\n                location = parameters.get('location', 'unknown')\n                return self.navigate_to_location(location)\n            elif action_type == 'grasp_object':\n                obj = parameters.get('object', 'unknown')\n                return self.grasp_object(obj)\n            elif action_type == 'locate_object':\n                obj = parameters.get('object', 'unknown')\n                return self.locate_object(obj)\n            elif action_type == 'approach_object':\n                obj = parameters.get('object', 'unknown')\n                return self.approach_object(obj)\n            elif action_type == 'stop_robot':\n                return self.stop_robot()\n            elif action_type == 'perform_action':\n                action_name = parameters.get('action', 'unknown')\n                return self.perform_action(action_name)\n            elif action_type == 'confirm_arrival':\n                location = parameters.get('location', 'unknown')\n                return self.confirm_arrival(location)\n            elif action_type == 'confirm_grasp':\n                return self.confirm_grasp()\n            elif action_type == 'confirm_completion':\n                return self.confirm_completion()\n            elif action_type == 'wait_for_response':\n                duration = parameters.get('duration', 1.0)\n                time.sleep(duration)\n                return True\n            elif action_type == 'request_clarification':\n                command = parameters.get('command', '')\n                return self.request_clarification(command)\n            else:\n                self.get_logger().warn(f'Unknown action type: {action_type}')\n                return False\n\n        except Exception as e:\n            self.get_logger().error(f'Error executing action {action_type}: {e}')\n            return False\n\n    def speak_text(self, text: str) -> bool:\n        \"\"\"Speak text using text-to-speech\"\"\"\n        self.get_logger().info(f'Speaking: {text}')\n        # In real implementation, this would use a TTS system\n        # For now, we'll just log it\n        return True\n\n    def navigate_to_location(self, location: str) -> bool:\n        \"\"\"Navigate to a specific location\"\"\"\n        self.get_logger().info(f'Navigating to {location}')\n        # In real implementation, this would send navigation commands\n        cmd_vel = Twist()\n        cmd_vel.linear.x = 0.2  # Move forward\n        self.cmd_vel_pub.publish(cmd_vel)\n        time.sleep(2.0)  # Simulate navigation time\n        self.stop_robot()\n        return True\n\n    def grasp_object(self, obj: str) -> bool:\n        \"\"\"Grasp a specific object\"\"\"\n        self.get_logger().info(f'Grasping {obj}')\n        # In real implementation, this would send manipulation commands\n        joint_cmd = Float32MultiArray()\n        joint_cmd.data = [0.1, 0.1, 0.1, 0.1, 0.1]  # Example joint positions\n        self.joint_cmd_pub.publish(joint_cmd)\n        time.sleep(1.0)  # Simulate grasping time\n        return True\n\n    def locate_object(self, obj: str) -> bool:\n        \"\"\"Locate an object using perception system\"\"\"\n        self.get_logger().info(f'Locating {obj}')\n        # In real implementation, this would use computer vision\n        return True\n\n    def approach_object(self, obj: str) -> bool:\n        \"\"\"Approach an object\"\"\"\n        self.get_logger().info(f'Approaching {obj}')\n        # In real implementation, this would navigate to object location\n        cmd_vel = Twist()\n        cmd_vel.linear.x = 0.1  # Move slowly forward\n        self.cmd_vel_pub.publish(cmd_vel)\n        time.sleep(1.0)\n        self.stop_robot()\n        return True\n\n    def stop_robot(self) -> bool:\n        \"\"\"Stop robot movement\"\"\"\n        cmd_vel = Twist()\n        self.cmd_vel_pub.publish(cmd_vel)\n        self.get_logger().info('Robot stopped')\n        return True\n\n    def perform_action(self, action_name: str) -> bool:\n        \"\"\"Perform a specific action (dance, wave, etc.)\"\"\"\n        self.get_logger().info(f'Performing action: {action_name}')\n        # In real implementation, this would execute predefined behaviors\n        return True\n\n    def confirm_arrival(self, location: str) -> bool:\n        \"\"\"Confirm arrival at location\"\"\"\n        self.get_logger().info(f'Confirmed arrival at {location}')\n        return True\n\n    def confirm_grasp(self) -> bool:\n        \"\"\"Confirm successful grasp\"\"\"\n        self.get_logger().info('Confirmed successful grasp')\n        return True\n\n    def confirm_completion(self) -> bool:\n        \"\"\"Confirm task completion\"\"\"\n        self.get_logger().info('Task completed successfully')\n        return True\n\n    def request_clarification(self, command: str) -> bool:\n        \"\"\"Request clarification for ambiguous command\"\"\"\n        self.get_logger().info(f'Requesting clarification for: {command}')\n        return self.speak_text(f\"I didn't understand '{command}'. Could you please repeat or rephrase?\")\n\n    def process_execution(self):\n        \"\"\"Final processing and user feedback\"\"\"\n        while rclpy.ok():\n            try:\n                execution_item = self.execution_queue.get(timeout=0.1)\n\n                # Update pipeline state\n                self.pipeline_state['current_stage'] = 'completed'\n\n                # Calculate and record latency\n                start_time = execution_item['plan'].timestamp\n                end_time = execution_item['timestamp']\n                latency = end_time - start_time\n                self.latency_measurements.append(latency)\n\n                # Send success feedback to user\n                feedback_msg = String()\n                feedback_msg.data = json.dumps({\n                    'event': 'command_completed',\n                    'command': execution_item['command'].text,\n                    'success': execution_item['success'],\n                    'latency': latency,\n                    'timestamp': execution_item['timestamp']\n                })\n                self.feedback_pub.publish(feedback_msg)\n\n                # Send response to user\n                response_msg = String()\n                if execution_item['success']:\n                    response_msg.data = f'Successfully executed: {execution_item[\"command\"].text}'\n                else:\n                    response_msg.data = f'Failed to execute: {execution_item[\"command\"].text}'\n                self.response_pub.publish(response_msg)\n\n                # Update success counter\n                if execution_item['success']:\n                    self.pipeline_state['success_count'] += 1\n                else:\n                    self.pipeline_state['error_count'] += 1\n\n                self.get_logger().info(f'Command completed: {execution_item[\"command\"].text} '\n                                     f'(Latency: {latency:.2f}s)')\n\n            except queue.Empty:\n                continue\n            except Exception as e:\n                self.get_logger().error(f'Error in execution processing: {e}')\n                self.handle_pipeline_error(e)\n\n    def handle_pipeline_error(self, error: Exception):\n        \"\"\"Handle pipeline errors and recovery\"\"\"\n        self.pipeline_state['error_count'] += 1\n\n        # Publish error feedback\n        feedback_msg = String()\n        feedback_msg.data = json.dumps({\n            'event': 'pipeline_error',\n            'error': str(error),\n            'stage': self.pipeline_state['current_stage'],\n            'timestamp': time.time()\n        })\n        self.feedback_pub.publish(feedback_msg)\n\n        self.get_logger().error(f'Pipeline error in {self.pipeline_state[\"current_stage\"]}: {error}')\n\n    def performance_monitor(self):\n        \"\"\"Monitor pipeline performance\"\"\"\n        if self.latency_measurements:\n            avg_latency = sum(self.latency_measurements) / len(self.latency_measurements)\n            self.pipeline_state['average_latency'] = avg_latency\n\n            # Keep only recent measurements (last 100)\n            if len(self.latency_measurements) > 100:\n                self.latency_measurements = self.latency_measurements[-100:]\n\n        # Publish performance status\n        status_msg = String()\n        status_data = {\n            'stage': self.pipeline_state['current_stage'],\n            'success_count': self.pipeline_state['success_count'],\n            'error_count': self.pipeline_state['error_count'],\n            'average_latency': self.pipeline_state['average_latency'],\n            'queue_sizes': {\n                'audio': self.audio_queue.qsize(),\n                'recognition': self.recognition_queue.qsize(),\n                'nlu': self.nlu_queue.qsize(),\n                'planning': self.planning_queue.qsize(),\n                'execution': self.execution_queue.qsize()\n            }\n        }\n        status_msg.data = json.dumps(status_data)\n        self.status_pub.publish(status_msg)\n\n    def execute_voice_command(self, goal_handle):\n        \"\"\"Execute voice command action (for external calls)\"\"\"\n        try:\n            command_text = goal_handle.request.command\n            self.get_logger().info(f'Executing external voice command: {command_text}')\n\n            # Create voice command object\n            voice_cmd = VoiceCommand(\n                text=command_text,\n                confidence=1.0,  # External commands have full confidence\n                timestamp=time.time()\n            )\n\n            # Process through pipeline by adding to recognition queue\n            # (since it's already text, skip ASR)\n            nlu_result = self.perform_nlu(command_text)\n            if nlu_result:\n                plan = self.generate_plan(nlu_result, command_text)\n                if plan:\n                    execution_success = self.execute_plan(plan)\n\n                    if execution_success:\n                        result = VoiceCommandResult()\n                        result.success = True\n                        result.message = f'Successfully executed: {command_text}'\n                        goal_handle.succeed()\n                    else:\n                        result = VoiceCommandResult()\n                        result.success = False\n                        result.message = f'Failed to execute: {command_text}'\n                        goal_handle.abort()\n                else:\n                    result = VoiceCommandResult()\n                    result.success = False\n                    result.message = f'Could not generate plan for: {command_text}'\n                    goal_handle.abort()\n            else:\n                result = VoiceCommandResult()\n                result.success = False\n                result.message = f'Could not understand command: {command_text}'\n                goal_handle.abort()\n\n        except Exception as e:\n            result = VoiceCommandResult()\n            result.success = False\n            result.message = f'Error processing command: {e}'\n            goal_handle.abort()\n\n        return result\n\nclass VoiceCommandAction:\n    \"\"\"Action definition for voice commands\"\"\"\n    pass\n\nclass VoiceCommandResult:\n    \"\"\"Result definition for voice commands\"\"\"\n    def __init__(self):\n        self.success = False\n        self.message = \"\"\n\nclass PipelineOptimizer:\n    \"\"\"Optimizer for voice-to-action pipeline performance\"\"\"\n\n    def __init__(self, node: VoiceToActionPipeline):\n        self.node = node\n        self.optimization_params = {\n            'adaptive_buffer_size': True,\n            'model_caching': True,\n            'pipeline_parallelism': True,\n            'resource_management': True,\n            'dynamic_batching': True\n        }\n\n    def optimize_pipeline(self):\n        \"\"\"Apply optimizations to improve pipeline performance\"\"\"\n        try:\n            # Optimize audio buffer sizes based on load\n            if self.optimization_params['adaptive_buffer_size']:\n                self.optimize_buffer_sizes()\n\n            # Optimize model caching\n            if self.optimization_params['model_caching']:\n                self.optimize_model_caching()\n\n            # Optimize pipeline parallelism\n            if self.optimization_params['pipeline_parallelism']:\n                self.optimize_parallelism()\n\n            # Optimize resource usage\n            if self.optimization_params['resource_management']:\n                self.optimize_resources()\n\n            self.node.get_logger().info('Pipeline optimizations applied')\n\n        except Exception as e:\n            self.node.get_logger().error(f'Error in pipeline optimization: {e}')\n\n    def optimize_buffer_sizes(self):\n        \"\"\"Optimize queue buffer sizes based on pipeline load\"\"\"\n        # Adjust buffer sizes based on queue utilization\n        avg_queue_size = (\n            self.node.audio_queue.qsize() +\n            self.node.recognition_queue.qsize() +\n            self.node.nlu_queue.qsize() +\n            self.node.planning_queue.qsize() +\n            self.node.execution_queue.qsize()\n        ) / 5\n\n        if avg_queue_size > 15:  # High load\n            # Increase buffer sizes to handle load\n            pass\n        elif avg_queue_size < 5:  # Low load\n            # Decrease buffer sizes to save memory\n            pass\n\n    def optimize_model_caching(self):\n        \"\"\"Optimize model caching for faster inference\"\"\"\n        # Preload frequently used models\n        pass\n\n    def optimize_parallelism(self):\n        \"\"\"Optimize pipeline parallelism\"\"\"\n        # Adjust thread pool sizes based on system capabilities\n        pass\n\n    def optimize_resources(self):\n        \"\"\"Optimize resource usage\"\"\"\n        # Monitor and adjust resource allocation\n        pass\n\ndef main(args=None):\n    \"\"\"Main function for complete voice-to-action system\"\"\"\n    rclpy.init(args=args)\n\n    # Create voice-to-action pipeline node\n    vta_pipeline = VoiceToActionPipeline()\n\n    # Create optimizer\n    optimizer = PipelineOptimizer(vta_pipeline)\n\n    try:\n        # Optimize pipeline periodically\n        import threading\n        def optimization_loop():\n            while rclpy.ok():\n                optimizer.optimize_pipeline()\n                time.sleep(10.0)  # Optimize every 10 seconds\n\n        optimization_thread = threading.Thread(target=optimization_loop, daemon=True)\n        optimization_thread.start()\n\n        # Run the pipeline\n        rclpy.spin(vta_pipeline)\n\n    except KeyboardInterrupt:\n        vta_pipeline.get_logger().info('Shutting down voice-to-action pipeline')\n    finally:\n        vta_pipeline.destroy_node()\n        rclpy.shutdown()\n\n# Example configuration for the voice-to-action system\n\"\"\"\n# voice_to_action_config.yaml\nvoice_to_action_pipeline:\n  ros__parameters:\n    # Audio processing parameters\n    audio:\n      sample_rate: 16000\n      chunk_size: 1024\n      channels: 1\n      vad_threshold: 0.01\n      silence_duration: 0.5\n\n    # Pipeline parameters\n    pipeline:\n      max_command_length: 100\n      confidence_threshold: 0.7\n      command_timeout: 10.0\n      pipeline_retry_limit: 3\n      response_timeout: 5.0\n      wake_word: \"robot\"\n      enable_confirmation: true\n\n    # Performance parameters\n    performance:\n      max_latency: 2.0\n      target_throughput: 5  # commands per minute\n      queue_sizes:\n        audio: 20\n        recognition: 20\n        nlu: 20\n        planning: 20\n        execution: 20\n\n    # Safety parameters\n    safety:\n      emergency_stop_keywords: [\"stop\", \"halt\", \"emergency\"]\n      maximum_velocity: 0.5\n      minimum_distance: 0.5\n\"\"\"\n\n# Example launch file for the voice-to-action system\n\"\"\"\n# voice_to_action_launch.py\nfrom launch import LaunchDescription\nfrom launch_ros.actions import Node\nfrom launch.actions import DeclareLaunchArgument\nfrom launch.substitutions import LaunchConfiguration\n\ndef generate_launch_description():\n    use_sim_time = LaunchConfiguration('use_sim_time', default='false')\n    config_file = LaunchConfiguration('config_file', default='voice_to_action_config.yaml')\n\n    return LaunchDescription([\n        # Voice-to-action pipeline\n        Node(\n            package='voice_to_action',\n            executable='voice_to_action_pipeline',\n            name='voice_to_action_pipeline',\n            parameters=[\n                {'use_sim_time': use_sim_time},\n                config_file\n            ],\n            output='screen'\n        ),\n\n        # Audio input node\n        Node(\n            package='audio_system',\n            executable='audio_input',\n            name='audio_input',\n            parameters=[\n                {'use_sim_time': use_sim_time}\n            ]\n        ),\n\n        # Camera input node (for context-aware commands)\n        Node(\n            package='image_common',\n            executable='camera_node',\n            name='camera',\n            parameters=[\n                {'use_sim_time': use_sim_time}\n            ]\n        )\n    ])\n\"\"\"\n"})})]})}function m(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(p,{...e})}):p(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>r,x:()=>s});var i=t(6540);const o={},a=i.createContext(o);function r(e){const n=i.useContext(a);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function s(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:r(e.components),i.createElement(a.Provider,{value:n},e.children)}}}]);