"use strict";(globalThis.webpackChunkphysical_ai_humanoid_robotics_book=globalThis.webpackChunkphysical_ai_humanoid_robotics_book||[]).push([[896],{5006:(e,n,s)=>{s.r(n),s.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>m,frontMatter:()=>t,metadata:()=>r,toc:()=>c});const r=JSON.parse('{"id":"module-1-robotic-nervous-system/chapter-1-6-sensor-integration","title":"Sensor Integration (Camera, LiDAR, IMU)","description":"Goal","source":"@site/docs/module-1-robotic-nervous-system/chapter-1-6-sensor-integration.md","sourceDirName":"module-1-robotic-nervous-system","slug":"/module-1-robotic-nervous-system/chapter-1-6-sensor-integration","permalink":"/physical-ai-humanoid-robotics-book/docs/module-1-robotic-nervous-system/chapter-1-6-sensor-integration","draft":false,"unlisted":false,"editUrl":"https://github.com/your-org/physical-ai-humanoid-robotics-book/tree/main/docs/module-1-robotic-nervous-system/chapter-1-6-sensor-integration.md","tags":[],"version":"current","frontMatter":{"id":"chapter-1-6-sensor-integration","title":"Sensor Integration (Camera, LiDAR, IMU)","sidebar_label":"Sensor Integration"},"sidebar":"book","previous":{"title":"Launch Files and Parameters","permalink":"/physical-ai-humanoid-robotics-book/docs/module-1-robotic-nervous-system/chapter-1-5-launch-files-parameters"},"next":{"title":"Physics Simulation Fundamentals","permalink":"/physical-ai-humanoid-robotics-book/docs/module-2-digital-twin/chapter-2-1-physics-simulation"}}');var i=s(4848),a=s(8453);const t={id:"chapter-1-6-sensor-integration",title:"Sensor Integration (Camera, LiDAR, IMU)",sidebar_label:"Sensor Integration"},o="Sensor Integration (Camera, LiDAR, IMU)",l={},c=[{value:"Goal",id:"goal",level:2},{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Overview",id:"overview",level:2},{value:"Key Concepts",id:"key-concepts",level:2},{value:"Step-by-Step Breakdown",id:"step-by-step-breakdown",level:2},{value:"Code Examples",id:"code-examples",level:2},{value:"Diagrams",id:"diagrams",level:2},{value:"Case Study",id:"case-study",level:2},{value:"References",id:"references",level:2},{value:"Review Questions",id:"review-questions",level:2},{value:"Practical Exercises",id:"practical-exercises",level:2}];function d(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,a.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"sensor-integration-camera-lidar-imu",children:"Sensor Integration (Camera, LiDAR, IMU)"})}),"\n",(0,i.jsx)(n.h2,{id:"goal",children:"Goal"}),"\n",(0,i.jsx)(n.p,{children:"Integrate various sensors commonly used in humanoid robots, including cameras, LiDAR, and IMUs, into ROS 2 systems with proper data processing and calibration."}),"\n",(0,i.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Integrate camera sensors and process image data in ROS 2"}),"\n",(0,i.jsx)(n.li,{children:"Connect and utilize LiDAR sensors for environment perception"}),"\n",(0,i.jsx)(n.li,{children:"Integrate IMU sensors for orientation and balance information"}),"\n",(0,i.jsx)(n.li,{children:"Understand sensor data processing pipelines in ROS 2"}),"\n",(0,i.jsx)(n.li,{children:"Implement sensor fusion techniques for humanoid robotics"}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,i.jsx)(n.p,{children:"Sensor integration is critical for humanoid robots to perceive and interact with their environment. Cameras provide visual information, LiDAR offers precise distance measurements, and IMUs give orientation and acceleration data. Proper integration of these sensors in ROS 2 requires understanding of sensor drivers, data formats, calibration procedures, and processing pipelines. For humanoid robots, sensor fusion is particularly important for balance, navigation, and interaction with the environment."}),"\n",(0,i.jsx)(n.h2,{id:"key-concepts",children:"Key Concepts"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"sensor_msgs"}),": Standard message types for sensor data in ROS 2"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"image_transport"}),": Framework for efficient image data transmission"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"tf2"}),": Transform library for coordinate frame management"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Calibration"}),": Process of determining sensor parameters and relationships"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Sensor Fusion"}),": Combining data from multiple sensors for better perception"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Driver Nodes"}),": ROS 2 nodes that interface with physical sensors"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"step-by-step-breakdown",children:"Step-by-Step Breakdown"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Camera Integration"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Use camera drivers that publish sensor_msgs/Image"}),"\n",(0,i.jsx)(n.li,{children:"Implement image processing pipelines"}),"\n",(0,i.jsx)(n.li,{children:"Use image_transport for efficient transmission"}),"\n",(0,i.jsx)(n.li,{children:"Apply camera calibration for accurate measurements"}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"LiDAR Integration"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Connect LiDAR sensors via USB, Ethernet, or serial"}),"\n",(0,i.jsx)(n.li,{children:"Process LaserScan or PointCloud2 messages"}),"\n",(0,i.jsx)(n.li,{children:"Implement obstacle detection and mapping"}),"\n",(0,i.jsx)(n.li,{children:"Handle different LiDAR types (2D, 3D, multi-layer)"}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"IMU Integration"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Process sensor_msgs/Imu messages"}),"\n",(0,i.jsx)(n.li,{children:"Understand orientation, angular velocity, and linear acceleration"}),"\n",(0,i.jsx)(n.li,{children:"Implement sensor fusion for attitude estimation"}),"\n",(0,i.jsx)(n.li,{children:"Use IMU data for balance control in humanoid robots"}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Coordinate Frame Management"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Use tf2 for managing sensor positions and orientations"}),"\n",(0,i.jsx)(n.li,{children:"Establish proper transform chains"}),"\n",(0,i.jsx)(n.li,{children:"Transform data between coordinate frames"}),"\n",(0,i.jsx)(n.li,{children:"Handle dynamic transforms for moving sensors"}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Sensor Calibration"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Camera intrinsic and extrinsic calibration"}),"\n",(0,i.jsx)(n.li,{children:"LiDAR calibration for multi-sensor systems"}),"\n",(0,i.jsx)(n.li,{children:"IMU bias and alignment calibration"}),"\n",(0,i.jsx)(n.li,{children:"Multi-sensor calibration procedures"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"code-examples",children:"Code Examples"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'# Example sensor integration node for humanoid robot\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image, LaserScan, Imu, PointCloud2\nfrom geometry_msgs.msg import Vector3, Quaternion\nfrom tf2_ros import TransformBroadcaster\nfrom cv_bridge import CvBridge\nimport cv2\nimport numpy as np\n\nclass SensorIntegrator(Node):\n    def __init__(self):\n        super().__init__(\'sensor_integrator\')\n\n        # Initialize CV bridge for image processing\n        self.cv_bridge = CvBridge()\n\n        # Create subscribers for different sensor types\n        self.image_sub = self.create_subscription(\n            Image,\n            \'/camera/image_raw\',\n            self.image_callback,\n            10\n        )\n\n        self.lidar_sub = self.create_subscription(\n            LaserScan,\n            \'/scan\',\n            self.lidar_callback,\n            10\n        )\n\n        self.imu_sub = self.create_subscription(\n            Imu,\n            \'/imu/data\',\n            self.imu_callback,\n            10\n        )\n\n        # Create publisher for processed sensor data\n        self.fused_sensor_pub = self.create_publisher(\n            PointCloud2,\n            \'/fused_pointcloud\',\n            10\n        )\n\n        # Initialize transform broadcaster\n        self.tf_broadcaster = TransformBroadcaster(self)\n\n        self.get_logger().info(\'Sensor Integrator initialized\')\n\n    def image_callback(self, msg):\n        """Process camera image data"""\n        try:\n            # Convert ROS Image message to OpenCV image\n            cv_image = self.cv_bridge.imgmsg_to_cv2(msg, "bgr8")\n\n            # Perform image processing (e.g., object detection)\n            # This is where you\'d implement your computer vision algorithms\n            processed_image = self.process_image(cv_image)\n\n            # Publish processed data or publish transforms\n            self.get_logger().info(f\'Received image: {msg.width}x{msg.height}\')\n\n        except Exception as e:\n            self.get_logger().error(f\'Error processing image: {e}\')\n\n    def lidar_callback(self, msg):\n        """Process LiDAR scan data"""\n        # Process LaserScan message\n        ranges = np.array(msg.ranges)\n\n        # Filter out invalid measurements\n        valid_ranges = ranges[np.isfinite(ranges)]\n\n        if len(valid_ranges) > 0:\n            min_distance = np.min(valid_ranges)\n            self.get_logger().info(f\'Min distance: {min_distance:.2f}m\')\n\n            # Implement obstacle detection logic here\n            obstacles = self.detect_obstacles(ranges, msg.angle_min, msg.angle_increment)\n\n    def imu_callback(self, msg):\n        """Process IMU data for balance and orientation"""\n        # Extract orientation from IMU\n        orientation = msg.orientation\n        angular_velocity = msg.angular_velocity\n        linear_acceleration = msg.linear_acceleration\n\n        # Calculate roll, pitch, yaw from quaternion\n        roll, pitch, yaw = self.quaternion_to_euler(\n            orientation.x, orientation.y, orientation.z, orientation.w\n        )\n\n        # For humanoid robots, use IMU data for balance control\n        balance_info = {\n            \'roll\': roll,\n            \'pitch\': pitch,\n            \'yaw\': yaw,\n            \'angular_velocity\': (angular_velocity.x, angular_velocity.y, angular_velocity.z),\n            \'linear_acceleration\': (linear_acceleration.x, linear_acceleration.y, linear_acceleration.z)\n        }\n\n        self.get_logger().info(f\'Orientation - Roll: {roll:.2f}, Pitch: {pitch:.2f}, Yaw: {yaw:.2f}\')\n\n    def process_image(self, cv_image):\n        """Placeholder for image processing logic"""\n        # Example: Convert to grayscale\n        gray = cv2.cvtColor(cv_image, cv2.COLOR_BGR2GRAY)\n        return gray\n\n    def detect_obstacles(self, ranges, angle_min, angle_increment):\n        """Detect obstacles from LiDAR data"""\n        angles = np.arange(len(ranges)) * angle_increment + angle_min\n        valid_indices = np.isfinite(ranges)\n\n        if np.any(valid_indices):\n            valid_ranges = ranges[valid_indices]\n            valid_angles = angles[valid_indices]\n\n            # Detect obstacles within a certain range (e.g., 1 meter)\n            obstacle_indices = valid_ranges < 1.0\n            if np.any(obstacle_indices):\n                obstacle_angles = valid_angles[obstacle_indices]\n                self.get_logger().info(f\'Found {len(obstacle_angles)} obstacles\')\n                return list(zip(obstacle_angles, valid_ranges[obstacle_indices]))\n\n        return []\n\n    def quaternion_to_euler(self, x, y, z, w):\n        """Convert quaternion to Euler angles (roll, pitch, yaw)"""\n        # Convert quaternion to Euler angles\n        t0 = +2.0 * (w * x + y * z)\n        t1 = +1.0 - 2.0 * (x * x + y * y)\n        roll = np.arctan2(t0, t1)\n\n        t2 = +2.0 * (w * y - z * x)\n        t2 = +1.0 if t2 > +1.0 else t2\n        t2 = -1.0 if t2 < -1.0 else t2\n        pitch = np.arcsin(t2)\n\n        t3 = +2.0 * (w * z + x * y)\n        t4 = +1.0 - 2.0 * (y * y + z * z)\n        yaw = np.arctan2(t3, t4)\n\n        return roll, pitch, yaw\n\ndef main(args=None):\n    rclpy.init(args=args)\n    sensor_integrator = SensorIntegrator()\n\n    try:\n        rclpy.spin(sensor_integrator)\n    except KeyboardInterrupt:\n        sensor_integrator.get_logger().info(\'Interrupted by user\')\n    finally:\n        sensor_integrator.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,i.jsx)(n.h2,{id:"diagrams",children:"Diagrams"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"Sensor Integration Architecture:\n\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   Camera    \u2502    \u2502   LiDAR     \u2502    \u2502    IMU      \u2502\n\u2502             \u2502    \u2502             \u2502    \u2502             \u2502\n\u2502 sensor_msgs \u2502    \u2502 sensor_msgs \u2502    \u2502 sensor_msgs \u2502\n\u2502    /Image   \u2502    \u2502   /LaserScan\u2502    \u2502    /Imu     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n      \u2502                  \u2502                  \u2502\n      \u2502                  \u2502                  \u2502\n      \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                         \u2502\n              \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n              \u2502   Sensor Integrator \u2502\n              \u2502      Node           \u2502\n              \u2502 (processes sensor   \u2502\n              \u2502  data and fuses     \u2502\n              \u2502  information)       \u2502\n              \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                         \u2502\n              \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n              \u2502   tf2 Transform     \u2502\n              \u2502    Management       \u2502\n              \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                         \u2502\n              \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n              \u2502  Processed Sensor   \u2502\n              \u2502     Data (e.g.,     \u2502\n              \u2502   PointCloud2)      \u2502\n              \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n"})}),"\n",(0,i.jsx)(n.h2,{id:"case-study",children:"Case Study"}),"\n",(0,i.jsx)(n.p,{children:"In humanoid robotics, sensor integration is critical for balance and navigation. For example, the ATLAS robot uses IMU data for balance control, LiDAR for environment mapping, and cameras for object recognition. The sensor data is fused to create a comprehensive understanding of the robot's state and environment. The IMU provides real-time feedback for the balance controller, while the LiDAR and cameras help the robot navigate and interact with objects in its environment."}),"\n",(0,i.jsx)(n.h2,{id:"references",children:"References"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"https://docs.ros.org/en/humble/p/sensor_msgs/",children:"sensor_msgs Package"})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"https://docs.ros.org/en/humble/p/image_transport/",children:"image_transport Package"})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"https://docs.ros.org/en/humble/p/tf2/",children:"tf2 Documentation"})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"https://docs.ros.org/en/humble/Tutorials/Advanced/Simulated-Sensors.html",children:"Camera Calibration"})}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"review-questions",children:"Review Questions"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsx)(n.li,{children:"What are the main sensor_msgs types used for camera, LiDAR, and IMU data?"}),"\n",(0,i.jsx)(n.li,{children:"How does tf2 help in sensor integration for humanoid robots?"}),"\n",(0,i.jsx)(n.li,{children:"Explain the difference between LaserScan and PointCloud2 message types."}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"practical-exercises",children:"Practical Exercises"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsx)(n.li,{children:"Create a node that subscribes to camera image data and performs basic processing"}),"\n",(0,i.jsx)(n.li,{children:"Implement a simple obstacle detection algorithm using LiDAR data"}),"\n",(0,i.jsx)(n.li,{children:"Create a node that processes IMU data and calculates orientation"}),"\n",(0,i.jsx)(n.li,{children:"Integrate multiple sensors in a single launch file and visualize the data"}),"\n"]})]})}function m(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(d,{...e})}):d(e)}},8453:(e,n,s)=>{s.d(n,{R:()=>t,x:()=>o});var r=s(6540);const i={},a=r.createContext(i);function t(e){const n=r.useContext(a);return r.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:t(e.components),r.createElement(a.Provider,{value:n},e.children)}}}]);